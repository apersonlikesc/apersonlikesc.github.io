<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2018-08-20 文字判断游戏]]></title>
    <url>%2F2018%2F08%2F20%2F2018-08-20%2F</url>
    <content type="text"><![CDATA[开发时间花了两天时间前一天没有写日志总的来说,还行,主要还是框架的功劳今天pressed的判断上想了一天时间….(真菜主要是要press放在了loop中之后时放在外面监听,实现了press一次之后还要添加音乐,应该还算是可以吧…]]></content>
      <categories>
        <category>开发日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-19]]></title>
    <url>%2F2018%2F08%2F19%2F2018-08-19%2F</url>
    <content type="text"><![CDATA[scrapy json中文乱码可以在setting.py文件中修改默认的输出编码方式，只需要在setting.py中增加如下语句（默认似乎是没有指定的，所以要增加，如果默认有，就直接修改）FEED_EXPORT_ENCODING = &#39;utf-8&#39; rot13ROT13 (“rotate by 13 places”, sometimes hyphenated ROT-13) is a simple letter substitution cipher that replaces a letter with the 13th letter after it, in the alphabet. ROT13 is a special case of the Caesar cipher which was developed in ancient Rome. Because there are 26 letters (2×13) in the basic Latin alphabet, ROT13 is its own inverse; that is, to undo ROT13, the same algorithm is applied, so the same action can be used for encoding and decoding. The algorithm provides virtually no cryptographic security, and is often cited as a canonical example of weak encryption. ROT13 is used in online forums as a means of hiding spoilers, punchlines, puzzle solutions, and offensive materials from the casual glance. ROT13 has inspired a variety of letter and word games on-line, and is frequently mentioned in newsgroup conversations. 加密:tr &#39;A-Za-z&#39; &#39;N-ZA-Mn-za-m&#39; &lt;&lt;&lt; &quot;The Quick Brown Fox Jumps Over The Lazy Dog&quot; 解密:echo &quot;The Quick Brown Fox Jumps Over The Lazy Dog&quot; |tr &#39;N-ZA-Mn-za-m&#39; &#39;A-Za-z&#39; Hex dump(十六进制转储)In computing,a hex dump is a hexadecimal view (on screen or paper) of computer data, from RAM or from a file or storage device. Looking at a hex dump of data is commonly done as a part of debugging, or of reverse engineering. In a hex dump, each byte (8-bits) is represented as a two-digit hexadecimal number.(每个字节==&gt;两个十六进制的数) Hex dumps are commonly organized into rows of 8 or 16 bytes, sometimes separated by whitespaces. Some hex dumps have the hexadecimal memory address at the beginning and/or a checksum byte at the end of each line. Although the name implies the use of base-16 output, some hex dumping software may have options for base-8 (octal) or base-10 (decimal) output. Some common names for this program function are hexdump, od, xxd and simply dump or even D. xxd 得到文件的头 得到的xxd文件要先-r出原来的文件哦 解压方式gzip -d(gunzip)tar -zxvftar -xvfbzip2 -d]]></content>
      <categories>
        <category>杂食动物</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh教程(another)]]></title>
    <url>%2F2018%2F08%2F19%2Fssh2%2F</url>
    <content type="text"><![CDATA[ssh(another page)Public and Private KeysPublic key authentication is more secure than password authentication. This is particularly important if the computer is visible on the internet. If you don’t think it’s important, try logging the login attempts you get for the next week. My computer - a perfectly ordinary desktop PC - had over 4,000 attempts to guess my password and almost 2,500 break-in attempts in the last week alone. With public key authentication, the authenticating entity has a public key and a private key. Each key is a large number with special mathematical properties. The private key is kept on the computer you log in from, while the public key is stored on the .ssh/authorized_keys file on all the computers you want to log in to. When you log in to a computer, the SSH server uses the public key to “lock” messages in a way that can only be “unlocked” by your private key - this means that even the most resourceful attacker can’t snoop(探听) on, or interfere(接入) with, your session. As an extra security measure, most SSH programs store the private key in a passphrase-protected format, so that if your computer is stolen or broken in to, you should have enough time to disable your old public key before they break the passphrase and start using your key. Wikipedia has a more detailed explanation of how keys work. Public key authentication is a much better solution than passwords for most people. In fact, if you don’t mind leaving a private key unprotected on your hard disk, you can even use keys to do secure automatic log-ins - as part of a network backup, for example. Different SSH programs generate public keys in different ways, but they all generate public keys in a similar format:&lt;ssh-rsa or ssh-dss&gt; &lt;really long string of nonsense&gt; &lt;username&gt;@&lt;host&gt; Key-Based SSH LoginsKey-based authentication is the most secure of several modes of authentication usable with OpenSSH, such as plain password and Kerberos tickets(一种安全方式). Key-based authentication has several advantages over password authentication, for example the key values are significantly more difficult to brute-force, or guess than plain passwords, provided an ample key length. Other authentication methods are only used in very specific situations. SSH can use either “RSA” (Rivest-Shamir-Adleman)(人名) or “DSA” (“Digital Signature Algorithm”) keys. Both of these were considered state-of-the-art(最先进的) algorithms when SSH was invented, but DSA has come to be seen as less secure in recent years. RSA is the only recommended choice for new keys, so this guide uses “RSA key” and “SSH key” interchangeably. Key-based authentication uses two keys, one “public” key that anyone is allowed to see, and another “private” key that only the owner is allowed to see. To securely communicate using key-based authentication, one needs to create a key pair, securely store the private key on the computer one wants to log in from, and store the public key on the computer one wants to log in to. Using key based logins with ssh is generally considered more secure than using plain password logins. This section of the guide will explain the process of generating a set of public/private RSA keys, and using them for logging into your Ubuntu computer(s) via OpenSSH. Generating RSA KeysThe first step involves creating a set of RSA keys for use in authentication. This should be done on the client. To create your public and private SSH keys on the command-line:123mkdir ~/.sshchmod 700 ~/.sshssh-keygen -t rsa You will be prompted for a location to save the keys, and a passphrase for the keys. This passphrase will protect your private key while it’s stored on the hard drive:1234567Generating public/private rsa key pair.Enter file in which to save the key (/home/b/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /home/b/.ssh/id_rsa.Your public key has been saved in /home/b/.ssh/id_rsa.pub.Your public key is now available as .ssh/id_rsa.pub in your home folder. Congratulations! You now have a set of keys. Now it’s time to make your systems allow you to login with them Choosing a good passphraseYou need to change all your locks if your RSA key is stolen. Otherwise the thief could impersonate(扮演) you wherever you authenticate with that key. An SSH key passphrase is a secondary form of security that gives you a little time when your keys are stolen. If your RSA key has a strong passphrase, it might take your attacker a few hours to guess by brute force. That extra time should be enough to log in to any computers you have an account on, delete your old key from the .ssh/authorized_keys file, and add a new key. Your SSH key passphrase is only used to protect your private key from thieves. It’s never transmitted over the Internet, and the strength of your key has nothing to do with the strength of your passphrase. The decision to protect your key with a passphrase involves convenience x security. Note that if you protect your key with a passphrase, then when you type the passphrase to unlock it, your local computer will generally leave the key unlocked for a time. So if you use the key multiple times without logging out of your local account in the meantime, you will probably only have to type the passphrase once. If you do adopt a passphrase, pick a strong one and store it securely in a password manager. You may also write it down on a piece of paper and keep it in a secure place. If you choose not to protect the key with a passphrase, then just press the return when ssh-keygen asks. Key Encryption LevelNote: The default is a 2048 bit key. You can increase this to 4096 bits with the -b flag (Increasing the bits makes it harder to crack the key by brute force methods).ssh-keygen -t rsa -b 4096 Password AuthenticationThe main problem with public key authentication is that you need a secure way of getting the public key onto a computer before you can log in with it. If you will only ever use an SSH key to log in to your own computer from a few other computers (such as logging in to your PC from your laptop), you should copy your SSH keys over on a memory stick, and disable password authentication altogether. If you would like to log in from other computers from time to time (such as a friend’s PC), make sure you have a strong password. Transfer Client Key to HostThe key you need to transfer to the host is the public one. If you can log in to a computer over SSH using a password, you can transfer your RSA key by doing the following from your own computer:ssh-copy-id &lt;username&gt;@&lt;host&gt;Where &lt;username&gt; and &lt;host&gt; should be replaced by your username and the name of the computer you’re transferring your key to. (i) Due to this bug, you cannot specify a port other than the standard port 22. You can work around this by issuing the command like this: ssh-copy-id &quot;&lt;username&gt;@&lt;host&gt; -p &lt;port_nr&gt;&quot;. If you are using the standard port 22, you can ignore this tip. Another alternative is to copy the public key file to the server and concatenate it onto the authorized_keys file manually. It is wise to back that up first:12cp authorized_keys authorized_keys_Backupcat id_rsa.pub &gt;&gt; authorized_keys You can make sure this worked by doing:ssh &lt;username&gt;@&lt;host&gt;You should be prompted for the passphrase for your key:Enter passphrase for key ‘/home//.ssh/id_rsa’:Enter your passphrase, and provided host is configured to allow key-based logins, you should then be logged in as usual. TroubleshootingEncrypted Home DirectoryIf you have an encrypted home directory, SSH cannot access your authorized_keys file because it is inside your encrypted home directory and won’t be available until after you are authenticated. Therefore, SSH will default to password authentication. To solve this, create a folder outside your home named /etc/ssh/&lt;username&gt; (replace &quot;&lt;username&gt;&quot; with your actual username). This directory should have 755 permissions and be owned by the user. Move the authorized_keys file into it. The authorized_keys file should have 644 permissions and be owned by the user.Then edit your /etc/ssh/sshd_config and add:AuthorizedKeysFile /etc/ssh/%u/authorized_keysFinally, restart ssh with:sudo service ssh restartThe next time you connect with SSH you should not have to enter your password. username@host’s password:If you are not prompted for the passphrase, and instead get just theusername@host&#39;s password:prompt as usual with password logins, then read on. There are a few things which could prevent this from working as easily as demonstrated above. On default Ubuntu installs however, the above examples should work. If not, then check the following condition, as it is the most frequent cause: On the host computer, ensure that the /etc/ssh/sshd_config contains the following lines, and that they are uncommented;PubkeyAuthentication yesRSAAuthentication yesIf not, add them, or uncomment them, restart OpenSSH, and try logging in again. If you get the passphrase prompt now, then congratulations, you’re logging in with a key! Permission denied (publickey)If you’re sure you’ve correctly configured sshd_config, copied your ID, and have your private key in the .ssh directory, and still getting this error:Permission denied (publickey).Chances are, your /home/&lt;user&gt; or ~/.ssh/authorized_keys permissions are too open by OpenSSH standards. You can get rid of this problem by issuing the following commands: 123chmod go-w ~/chmod 700 ~/.sshchmod 600 ~/.ssh/authorized_keys Error: Agent admitted failure to sign using the key.This error occurs when the ssh-agent on the client is not yet managing the key. Issue the following commands to fix:ssh-addThis command should be entered after you have copied your public key to the host computer. Debugging and sorting out further problemsThe permissions of files and folders is crucial to this working. You can get debugging information from both the client and server.if you think you have set it up correctly , yet still get asked for the password, try starting the server with debugging output to the terminal.sudo /usr/sbin/sshd -dTo connect and send information to the client terminalssh -v ( or -vv) username@host&#39;s Where to From Here?No matter how your public key was generated, you can add it to your Ubuntu system by opening the file .ssh/authorized_keys in your favourite text editor and adding the key to the bottom of the file. You can also limit the SSH features that the key can use, such as disallowing port-forwarding or only allowing a specific command to be run. This is done by adding “options” before the SSH key, on the same line in the authorized_keys file. For example, if you maintain a CVS repository, you could add a line like this:command=&quot;/usr/bin/cvs server&quot;,no-agent-forwarding,no-port-forwarding,no-X11-forwarding,no-user-rc ssh-dss &lt;string of nonsense&gt;...When the user with the specified key logged in, the server would automatically run /usr/bin/cvs server, ignoring any requests from the client to run another command such as a shell. For more information, see the sshd man page.]]></content>
      <categories>
        <category>arch</category>
      </categories>
      <tags>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh教程]]></title>
    <url>%2F2018%2F08%2F18%2Fssh%2F</url>
    <content type="text"><![CDATA[sshIf you’re connecting to another computer over the Internet, you’ll probably want to keep your data safe. SSH is one way to help do that. To make it happen, you’ll need to set up SSH properly on your computer, and then create an encrypted(加密) connection to your server. Just remember, in order for the connection to be secure, both ends of the connection need to have SSH enabled. Follow this guide to make sure that your connection is as safe as possible. Connecting for the First TimeInstall SSH.For Windows, you will need to download and install an SSH client program. The most popular one is Cygwin, which is available for free from the developer’s website. Download and install it like you would any other program. Another popular free program is PuTTY.During the Cygwin installation, you must choose to install OpenSSH from the Net section.Linux and Mac OS X come with SSH already installed on the system. This is because SSH is a UNIX system, and Linux and OS X are derived from UNIX.If you have Windows 10 with the Anniversary Update, you can install the Windows Subsystem for Linux which comes with SSH preinstalled. Run SSH.Open the terminal program that is installed by Cygwin, or Bash on Ubuntu on Windows for Windows 10, or open the Terminal in OS X or Linux. SSH uses the terminal interface to interact with other computers. There is no graphical interface for SSH, so you will need to get comfortable typing in commands. Test the connection.Before you dive into creating secure keys and moving files, you’ll want to test that SSH is properly configured on your computer as well as the system you are connecting to. Enter the following command, replacing with your username on the remote computer, and with the address for the remote computer or server:$ ssh &lt;username&gt;@&lt;remote&gt;You will be asked for your password once the connection is established. You will not see the cursor move or any characters input when you type your password.If this step fails, then either SSH is configured incorrectly on your computer or the remote computer is not accepting SSH connections. basic commands(a part)Copy files from your location to the remote computer. If you need to copy files from your local computer to the computer you are accessing remotely, you can use the scp command:scp /localdirectory/example1.txt &lt;username&gt;@&lt;remote&gt;:&lt;path&gt;will copy example1.txt to the specified on the remote computer. You can leave blank to copy to the root folder of the remote computer.scp &lt;username&gt;@&lt;remote&gt;:/home/example1.txt ./ will move example1.txt from the home directory on the remote computer to the current directory on the local computer. Creating Encrypted KeysCreate your SSH keys.These keys will allow you to connect to the remote location without having to enter your password each time. This is a much more secure way to connect to the remote computer, as the password will not have to transmitted over the network.Create the key folder on your computer by entering the command $ mkdir .sshCreate the public and private keys by using the command $ ssh-keygen –t rsaYou will be asked if you would like to create a passphrase for the keys; this is optional. If you don’t want to create a passphrase, press Enter. This will create two keys in the .ssh directory: id_rsa and id_rsa.pubChange your private key’s permissions. In order to ensure that the private key is only readable by you, enter the command $ chmod 600 .ssh/id_rsa Place the public key on the remote computer.Once your keys are created, you’re ready to place the public key on the remote computer so that you can connect without a password. Enter the following command, replacing the appropriate parts as explained earlier:$ scp .ssh/id_rsa.pub &lt;username&gt;@&lt;remote&gt;:Make sure to include the colon (:) at the end of the command.You will be asked to input your password before the file transfer starts. Install the public key on the remote computer.Once you’ve placed the key on the remote computer, you will need to install it so that it works correctly. First, log in to the remote computer the same way.Create an SSH folder on the remote computer, if it does not already exist: $ mkdir .sshAppend your key to the authorized keys file. If the file does not exist yet, it will be created: $ cat id_rsa.pub &gt;&gt; .ssh/authorized_keysChange the permissions for the SSH folder to allow access: $ chmod 700 .ssh Check that the connection works.Once the key has been installed on the remote computer, you should be able to initiate a connection without being asked to enter your password. Enter the following command to test the connection: $ ssh &lt;username&gt;@&lt;remote&gt;If you connect without being prompted for the password, then the keys are configured correctly.]]></content>
      <categories>
        <category>arch</category>
      </categories>
      <tags>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-18]]></title>
    <url>%2F2018%2F08%2F18%2F2018-08-18%2F</url>
    <content type="text"><![CDATA[SQL注入所谓SQL注入，就是通过把SQL命令插入到Web表单提交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令。select * from username = ____ and password=_____select * from username &quot;test&quot; or &quot;&quot;=&quot;&quot; and password=&quot;123456&quot; XSSXSS则是攻击者往Web页面里插入恶意Script代码，当用户浏览该页之时，嵌入Web里面的Script代码会被执行，从而达到恶意攻击用户的目的。&lt;p style=&#39;color:red&#39;&gt;你好啊，尊敬的______&lt;p&gt;&lt;p style=&#39;color:red&#39;&gt;你好啊，尊敬的 xxx&lt;script&gt;alert(1)&lt;/script&gt;&lt;p&gt; 远程命令执行而远程命令执行，是用户通过浏览器提交执行命令，由于服务器端没有针对执行函数做过滤，导致执行命令。ping ___ping www.baidu.com &amp; wget xxxxxxxxxxx 越权越权漏洞是比较常见的漏洞类型，越权漏洞可以理解为，一个正常的用户A通常只能够对自己的一些信息进行增删改查，但是由于程序员的一时疏忽 ，对信息进行增删改查的时候没有进行一个判断，判断所需要操作的信息是否属于对应的用户，可以导致用户A可以操作其他人的信息。Cookie: uid=11426;Cookie: uid=1;]]></content>
      <categories>
        <category>杂食动物</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy Pass 4]]></title>
    <url>%2F2018%2F08%2F17%2Fscrapy4%2F</url>
    <content type="text"><![CDATA[Command line toolNew in version 0.10. Scrapy is controlled through the scrapy command-line tool, to be referred here as the “Scrapy tool” to differentiate it from the sub-commands, which we just call “commands” or “Scrapy commands”. The Scrapy tool provides several commands, for multiple purposes, and each one accepts a different set of arguments and options. Configuration settingsScrapy will look for configuration parameters in ini-style scrapy.cfg files in standard locations: /etc/scrapy.cfg or c:\scrapy\scrapy.cfg (system-wide), ~/.config/scrapy.cfg ($XDG_CONFIG_HOME) and ~/.scrapy.cfg ($HOME) for global (user-wide) settings, and scrapy.cfg inside a scrapy project’s root (see next section).Settings from these files are merged in the listed order of preference: user-defined values have higher priority than system-wide defaults and project-wide settings will override all others, when defined.(优先级依次递增)Scrapy also understands, and can be configured through, a number of environment variables. Currently these are: SCRAPY_SETTINGS_MODULE (see Designating the settings)SCRAPY_PROJECTSCRAPY_PYTHON_SHELL (see Scrapy shell) Default structure of Scrapy projectsBefore delving into the command-line tool and its sub-commands, let’s first understand the directory structure of a Scrapy project. Though it can be modified, all Scrapy projects have the same file structure by default, similar to this:123456789101112scrapy.cfgmyproject/ __init__.py items.py middlewares.py pipelines.py settings.py spiders/ __init__.py spider1.py spider2.py ... The directory where the scrapy.cfg file resides is known as the project root directory. That file contains the name of the python module that defines the project settings. Here is an example:12[settings]default = myproject.settings Using the scrapy toolYou can start by running the Scrapy tool with no arguments and it will print some usage help and the available commands:123456789Scrapy X.Y - no active projectUsage: scrapy &lt;command&gt; [options] [args]Available commands: crawl Run a spider fetch Fetch a URL using the Scrapy downloader[...] The first line will print the currently active project if you’re inside a Scrapy project. In this example it was run from outside a project. If run from inside a project it would have printed something like this:123456Scrapy X.Y - project: myprojectUsage: scrapy &lt;command&gt; [options] [args][...] Creating projectsThe first thing you typically do with the scrapy tool is create your Scrapy project:scrapy startproject myproject [project_dir]That will create a Scrapy project under the project_dir directory. If project_dir wasn’t specified, project_dir will be the same as myproject.Next, you go inside the new project directory:cd project_dirAnd you’re ready to use the scrapy command to manage and control your project from there(project_dir). Controlling projectsYou use the scrapy tool from inside your projects to control and manage them.For example, to create a new spider:scrapy genspider mydomain mydomain.comSome Scrapy commands (like crawl) must be run from inside a Scrapy project. Also keep in mind that some commands may have slightly different behaviours when running them from inside projects. For example, the fetch command will use spider-overridden behaviours (such as the user_agent attribute to override the user-agent) if the url being fetched is associated with some specific spider. This is intentional, as the fetch command is meant to be used to check how spiders are downloading pages.(注意某一些特殊命令会有不同的行为,这不废话吗) Available tool commandsThis section contains a list of the available built-in commands with a description and some usage examples. Remember, you can always get more info about each command by running:scrapy &lt;command&gt; -hAnd you can see all available commands with:scrapy -hThere are two kinds of commands, those that only work from inside a Scrapy project (Project-specific commands) and those that also work without an active Scrapy project (Global commands), though they may behave slightly different when running from inside a project (as they would use the project overridden settings). Global commands:12345678startprojectgenspidersettingsrunspidershellfetchviewversion Project-only commands:123456crawlchecklisteditparsebench startproject(创建项目) Syntax: scrapy startproject &lt;project_name&gt; [project_dir]Requires project: no Creates a new Scrapy project named project_name, under the project_dir directory. If project_dir wasn’t specified, project_dir will be the same as project_name. Usage example:$ scrapy startproject myproject genspider Syntax: scrapy genspider [-t template] Requires project: no Create a new spider in the current folder or in the current project’s spiders folder, if called from inside a project. The parameter is set as the spider’s name, while is used to generate the allowed_domains and start_urls spider’s attributes. Usage example:123456789101112$ scrapy genspider -lAvailable templates: basic crawl csvfeed xmlfeed$ scrapy genspider example example.comCreated spider &apos;example&apos; using template &apos;basic&apos;$ scrapy genspider -t crawl scrapyorg scrapy.orgCreated spider &apos;scrapyorg&apos; using template &apos;crawl&apos; This is just a convenience shortcut command for creating spiders based on pre-defined templates, but certainly not the only way to create spiders. You can just create the spider source code files yourself, instead of using this command. crawl Syntax: scrapy crawl Requires project: yesStart crawling using a spider. Usage examples:12$ scrapy crawl myspider[ ... myspider starts crawling ... ] check Syntax: scrapy check [-l] Requires project: yesRun contract checks.(链接测试) Usage examples:1234567891011121314$ scrapy check -lfirst_spider * parse * parse_itemsecond_spider * parse * parse_item$ scrapy check[FAILED] first_spider:parse_item&gt;&gt;&gt; &apos;RetailPricex&apos; field is missing[FAILED] first_spider:parse&gt;&gt;&gt; Returned 92 requests, expected 0..4 list(列出项目中的所有spider) Syntax: scrapy listRequires project: yesList all available spiders in the current project. The output is one spider per line. Usage example:123$ scrapy listspider1spider2 edit(编辑) Syntax: scrapy edit Requires project: yesEdit the given spider using the editor defined in the EDITOR environment variable or (if unset) the EDITOR setting.(我觉得我可能不会用到) This command is provided only as a convenience shortcut for the most common case, the developer is of course free to choose any tool or IDE to write and debug spiders. Usage example:1$ scrapy edit spider1 fetch (下载页面代码) Syntax: scrapy fetch Requires project: noDownloads the given URL using the Scrapy downloader and writes the contents to standard output. The interesting thing about this command is that it fetches the page how the spider would download it. For example, if the spider has a USER_AGENT attribute which overrides the User Agent, it will use that one. So this command can be used to “see” how your spider would fetch a certain page. If used outside a project, no particular per-spider behaviour would be applied and it will just use the default Scrapy downloader settings. Supported options: --spider=SPIDER: bypass spider autodetection and force use of specific spider--headers: print the response’s HTTP headers instead of the response’s body--no-redirect: do not follow HTTP 3xx redirects (default is to follow them) Usage examples:12345678910111213$ scrapy fetch --nolog http://www.example.com/some/page.html[ ... html content here ... ]$ scrapy fetch --nolog --headers http://www.example.com/&#123;&apos;Accept-Ranges&apos;: [&apos;bytes&apos;], &apos;Age&apos;: [&apos;1263 &apos;], &apos;Connection&apos;: [&apos;close &apos;], &apos;Content-Length&apos;: [&apos;596&apos;], &apos;Content-Type&apos;: [&apos;text/html; charset=UTF-8&apos;], &apos;Date&apos;: [&apos;Wed, 18 Aug 2010 23:59:46 GMT&apos;], &apos;Etag&apos;: [&apos;&quot;573c1-254-48c9c87349680&quot;&apos;], &apos;Last-Modified&apos;: [&apos;Fri, 30 Jul 2010 15:30:18 GMT&apos;], &apos;Server&apos;: [&apos;Apache/2.2.3 (CentOS)&apos;]&#125; view Syntax: scrapy view Requires project: noOpens the given URL in a browser, as your Scrapy spider would “see” it. Sometimes spiders see pages differently from regular users, so this can be used to check what the spider “sees” and confirm it’s what you expect. Supported options: --spider=SPIDER: bypass spider autodetection and force use of specific spider--no-redirect: do not follow HTTP 3xx redirects (default is to follow them)Usage example:12$ scrapy view http://www.example.com/some/page.html[ ... browser starts ... ] shell Syntax: scrapy shell [url]Requires project: noStarts the Scrapy shell for the given URL (if given) or empty if no URL is given. Also supports UNIX-style local file paths, either relative with ./ or ../ prefixes or absolute file paths. See Scrapy shell for more info. Supported options: --spider=SPIDER: bypass spider autodetection and force use of specific spider-c code(设置返回状态): evaluate the code in the shell, print the result and exit--no-redirect: do not follow HTTP 3xx redirects (default is to follow them); this only affects the URL you may pass as argument on the command line; once you are inside the shell, fetch(url) will still follow HTTP redirects by default.Usage example:1234567891011121314$ scrapy shell http://www.example.com/some/page.html[ ... scrapy shell starts ... ]$ scrapy shell --nolog http://www.example.com/ -c &apos;(response.status, response.url)&apos;(200, &apos;http://www.example.com/&apos;)# shell follows HTTP redirects by default$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c &apos;(response.status, response.url)&apos;(200, &apos;http://example.com/&apos;)# you can disable this with --no-redirect# (only for the URL passed as command line argument)$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c &apos;(response.status, response.url)&apos;(302, &apos;http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F&apos;) parse Syntax: scrapy parse [options]Requires project: yesFetches the given URL and parses it with the spider that handles it, using the method passed with the –callback option, or parse if not given. Supported options: --spider=SPIDER: bypass spider autodetection and force use of specific spider--a NAME=VALUE: set spider argument (may be repeated)--callback or -c: spider method to use as callback for parsing the response--meta or -m: additional request meta that will be passed to the callback request. This must be a valid json string. Example: –meta=’{“foo” : “bar”}’--pipelines: process items through pipelines--rules or -r: use CrawlSpider rules to discover the callback (i.e. spider method) to use for parsing the response--noitems: don’t show scraped items--nolinks: don’t show extracted links--nocolour: avoid using pygments to colorize the output--depth or -d: depth level for which the requests should be followed recursively (default: 1)--verbose or -v: display information for each depth levelUsage example:1234567891011$ scrapy parse http://www.example.com/ -c parse_item[ ... scrapy log lines crawling example.com spider ... ]&gt;&gt;&gt; STATUS DEPTH LEVEL 1 &lt;&lt;&lt;# Scraped Items ------------------------------------------------------------[&#123;&apos;name&apos;: u&apos;Example item&apos;, &apos;category&apos;: u&apos;Furniture&apos;, &apos;length&apos;: u&apos;12 cm&apos;&#125;]# Requests -----------------------------------------------------------------[] settings Syntax: scrapy settings [options]Requires project: noGet the value of a Scrapy setting. If used inside a project it’ll show the project setting value, otherwise it’ll show the default Scrapy value for that setting. Example usage:1234$ scrapy settings --get BOT_NAMEscrapybot$ scrapy settings --get DOWNLOAD_DELAY0 runspider(单纯的运行文件) Syntax: scrapy runspider &lt;spider_file.py&gt;Requires project: noRun a spider self-contained in a Python file, without having to create a project. Example usage:12$ scrapy runspider myspider.py[ ... spider starts crawling ... ] version Syntax: scrapy version [-v]Requires project: noPrints the Scrapy version. If used with -v it also prints Python, Twisted and Platform info, which is useful for bug reports. bench(估计用不着)New in version 0.17. Syntax: scrapy benchRequires project: noRun a quick benchmark test. Benchmarking.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-17]]></title>
    <url>%2F2018%2F08%2F17%2F2018-08-17%2F</url>
    <content type="text"><![CDATA[暴露真实IP会遭到DDos攻击(DDoS:Distributed Denial of Service)分布式拒绝攻击 Windows 用户“一般被默认授予管理员权限，那意味着他们几乎可以访问系统中的一切”。Linux，反而很好地限制了“root”权限 macos “通过隐匿实现的安全”，这秉承了“让软件内部运作保持专有，从而不为人知是抵御攻击的最好方法”的理念 Windows 的流行本身就是个问题，操作系统的安全性可能很大程度上依赖于装机用户量的规模。对于恶意软件作者来说，Windows 提供了大的施展平台。专注其中可以让他们的努力发挥最大作用。 手机IMEI码在手机拨号处输入”*#06#”得到手机IMEI码,相当于手机的身份证号码,移动运营商通过IMEI码分辨用户设备，追踪用户地理位置，记录用户拨打电话、发送短信、上网等行为。(可怕) python控制浏览器12import webbrowserwebbrowser.open(&apos;http://baidu.com&apos;) pychram 安装第三方库失败解决方法pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package ed2ked2k全称叫“eDonkey2000 network”(电驴???)，是一种文件共享网络，最初用于共享音乐、电影和软件。与多数文件共享网络一样，它是分布式的；文件基于P2P原理存放于用户的电脑上而不是存储于一个中枢服务器。 eDonkey客户端程序连接到这个网络来共享文件。而eDonkey服务器作为一个通讯中心，使用户在ed2k网络内查找文件。它的客户端和服务端可以工作于Windows、Macintosh(这是mac..汗颜)、Linux、UNIX操作系统。任何人 都可以作为服务器加入这个网络。由于服务器经常变化，客户端会经常更新它的服务器列表。eDonkey用混合MD4摘要算法检查来识别文件。这使ed2k网络可以将不同文件名的同一文件成功识别为一个文件，并使同一文件名的不同文件得以区分(厉害)。eDonkeyd的另一特性是：对大于9.8MB的文件，它在下载完成前将其分割；这将加速大型文件的发送。为了便于文件搜索，一些Web站点对比较热门的文件建立 ed2k链接 ，这些网站通常也提供热门服务器列表便于用户更新。 fork boomfork炸弹（fork bomb）在计算机领域中是一种利用系统调用fork（或其他等效的方式）进行的拒绝服务攻击。与病毒与蠕虫不同的是，fork炸弹没有传染性，而且fork炸弹会使对同时执行进程、程序数设限的系统无法执行新程序，对于不设限的系统则使之停止响应。fork炸弹通过进程递归式派生(fork，亦即自我复制)，以使系统拒绝服务甚至崩溃。 fork炸弹以极快的速度创建大量进程（进程数呈以2为底数的指数增长趋势），并以此消耗系统分配予进程的可用空间使进程表饱和，而系统在进程表饱和后就无法运行新程序，除非进程表中的某一进程终止；但由于fork炸弹程序所创建的所有实例都会不断探测空缺的进程槽并尝试取用以创建新进程，因而即使在某进程终止后也基本不可能运行新进程。fork炸弹生成的子程序在消耗进程表空间的同时也会占用CPU和内存，从而导致系统与现有进程运行速度放缓，响应时间也会随之大幅增加，以致于无法正常完成任务，从而使系统的正常运作受到严重影响。由于现代Unix操作系统普遍采用运用写实拷贝技术，fork炸弹通常不会使进程表饱和。除了恶意触发fork炸弹破坏的情况外，软件开发中有时也会不慎在程序中嵌入fork炸弹，如在用于监听网络套接字并行使客户端-服务器结构系统中服务器端职责的应用程序中可能需要无限地进行循环（loop）与派生（fork）操作（类似下节示例程序所示），而在这种情况下源代码内的细微错误就可能在测试中“引爆”fork炸弹。以下程序段就是由Jaromil所作的在类UNIX系统的shell环境下触发fork炸弹的shell脚本代码，总共只用了13个字符（包括空格）：:(){ :|:&amp; };:注解如下： :() # 定义函数,函数名为”:”,即每当输入”:”时就会自动调用{}内代码{ # “:”函数开始标识: # 用递归方式调用”:”函数本身| # 并用管道(pipe)将其输出引至…: # 另一次递归调用的”:”函数综上,”:|:”表示的即是每次调用函数”:”的时候就会生成两份拷贝&amp; # 调用间脱钩,以使最初的”:”函数被杀死后为其所调用的两个”:”函数还能继续执行} # “:”函数结束标识; # “:”函数定义结束后将要进行的操作…: # 调用”:”函数,”引爆”fork炸弹其中函数名“:”只是简化的一例，实际实现时可以随意设定，一个较易理解（将函数名替换为“forkbomb”）的版本如下：forkbomb(){ forkbomb|forkbomb &amp;} ; forkbomb(自身调用自身?….理解成死循环好了) Windows下则可以批处理命令如下实现：%0|%0 POSIX标准下的C与C++的实现：#include &lt;unistd.h&gt;int main(){while(1) fork();return0;} Perl语言的实现：fork while fork 在系统中成功“引爆”fork炸弹后，我们可重启来使系统恢复正常运行；而若要以手动的方法使fork炸弹“熄火”，那前提就是必须杀死fork炸弹产生的所有进程。为此我们可以考虑使用程序来杀死fork炸弹产生的进程，但由于这一般需要创建新进程，且由于fork炸弹一直在探测与占用进程槽与内存空间，因而这一方法几乎不可能实现，而且用kill命令杀死进程后，释放出的进程槽又会被余下的fork炸弹线程所产生的新进程占用，在Windows下，用户可以退出当前用户会话的方式使系统恢复正常，但此法奏效的前提是fork炸弹是在该用户的特定会话内触发的]]></content>
      <categories>
        <category>杂食动物</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy Pass 3]]></title>
    <url>%2F2018%2F08%2F16%2Fscrapy3%2F</url>
    <content type="text"><![CDATA[Learn Scrapy pass 3接上文Extract dataExtracting quotes and authorsNow that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page. Each quote in http://quotes.toscrape.comis represented by HTML elements that look like this:123456789101112131415&lt;div class=&quot;quote&quot;&gt; &lt;span class=&quot;text&quot;&gt;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&lt;/span&gt; &lt;span&gt; by &lt;small class=&quot;author&quot;&gt;Albert Einstein&lt;/small&gt; &lt;a href=&quot;/author/Albert-Einstein&quot;&gt;(about)&lt;/a&gt; &lt;/span&gt; &lt;div class=&quot;tags&quot;&gt; Tags: &lt;a class=&quot;tag&quot; href=&quot;/tag/change/page/1/&quot;&gt;change&lt;/a&gt; &lt;a class=&quot;tag&quot; href=&quot;/tag/deep-thoughts/page/1/&quot;&gt;deep-thoughts&lt;/a&gt; &lt;a class=&quot;tag&quot; href=&quot;/tag/thinking/page/1/&quot;&gt;thinking&lt;/a&gt; &lt;a class=&quot;tag&quot; href=&quot;/tag/world/page/1/&quot;&gt;world&lt;/a&gt; &lt;/div&gt;&lt;/div&gt; Let’s open up scrapy shell and play a bit to find out how to extract the data we want:$ scrapy shell &#39;http://quotes.toscrape.com&#39;We get a list of selectors for the quote HTML elements with:1&gt;&gt;&gt; response.css(&quot;div.quote&quot;) Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote:1&gt;&gt;&gt; quote = response.css(&quot;div.quote&quot;)[0] Now, let’s extract title, author and the tags from that quote using the quote object we just created:123456&gt;&gt;&gt; title = quote.css(&quot;span.text::text&quot;).extract_first()&gt;&gt;&gt; title&apos;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&apos;&gt;&gt;&gt; author = quote.css(&quot;small.author::text&quot;).extract_first()&gt;&gt;&gt; author&apos;Albert Einstein&apos; Given that the tags are a list of strings, we can use the .extract() method to get all of them:123&gt;&gt;&gt; tags = quote.css(&quot;div.tags a.tag::text&quot;).extract()&gt;&gt;&gt; tags[&apos;change&apos;, &apos;deep-thoughts&apos;, &apos;thinking&apos;, &apos;world&apos;] Having figured out how to extract each bit, we can now iterate over all the quotes elements and put them together into a Python dictionary:123456789&gt;&gt;&gt; for quote in response.css(&quot;div.quote&quot;):... text = quote.css(&quot;span.text::text&quot;).extract_first()... author = quote.css(&quot;small.author::text&quot;).extract_first()... tags = quote.css(&quot;div.tags a.tag::text&quot;).extract()... print(dict(text=text, author=author, tags=tags))&#123;&apos;tags&apos;: [&apos;change&apos;, &apos;deep-thoughts&apos;, &apos;thinking&apos;, &apos;world&apos;], &apos;author&apos;: &apos;Albert Einstein&apos;, &apos;text&apos;: &apos;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&apos;&#125;&#123;&apos;tags&apos;: [&apos;abilities&apos;, &apos;choices&apos;], &apos;author&apos;: &apos;J.K. Rowling&apos;, &apos;text&apos;: &apos;“It is our choices, Harry, that show what we truly are, far more than our abilities.”&apos;&#125; ... a few more of these, omitted for brevity&gt;&gt;&gt; Extracting data in our spiderLet’s get back to our spider. Until now, it doesn’t extract any data in particular, just saves the whole HTML page to a local file (额) . Let’s integrate the extraction logic(逻辑) above into our spider. A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the yield Python keyword in the callback, as you can see below:12345678910111213141516import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; start_urls = [ &apos;http://quotes.toscrape.com/page/1/&apos;, &apos;http://quotes.toscrape.com/page/2/&apos;, ] def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.css(&apos;small.author::text&apos;).extract_first(), &apos;tags&apos;: quote.css(&apos;div.tags a.tag::text&apos;).extract(), &#125; (yield 不断调用,似乎不用储存) If you run this spider, it will output the extracted data with the log:12342016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/page/1/&gt;&#123;&apos;tags&apos;: [&apos;life&apos;, &apos;love&apos;], &apos;author&apos;: &apos;André Gide&apos;, &apos;text&apos;: &apos;“It is better to be hated for what you are than to be loved for what you are not.”&apos;&#125;2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/page/1/&gt;&#123;&apos;tags&apos;: [&apos;edison&apos;, &apos;failure&apos;, &apos;inspirational&apos;, &apos;paraphrased&apos;], &apos;author&apos;: &apos;Thomas A. Edison&apos;, &apos;text&apos;: &quot;“I have not failed. I&apos;ve just found 10,000 ways that won&apos;t work.”&quot;&#125; Storing the scraped dataThe simplest way to store the scraped data is by using Feed exports, with the following command:scrapy crawl quotes -o quotes.json That will generate an quotes.json file containing all scraped items, serialized in JSON. For historic reasons, Scrapy appends to a given file instead of overwriting its contents. If you run this command twice without removing the file before the second time, you’ll end up with a broken JSON file.(两次使用会损坏json文件)You can also use other formats, like JSON Lines:scrapy crawl quotes -o quotes.jl The JSON Lines format is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like JQ to help doing that at the command-line. In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an Item Pipeline. A placeholder file for Item Pipelines has been set up for you when the project is created, in tutorial/pipelines.py.(预先创建好了pipelines文件) Though you don’t need to implement any item pipelines if you just want to store the scraped items. Following linksLet’s say, instead of just scraping the stuff from the first two pages from http://quotes.toscrape.com, you want quotes from all the pages in the website. Now that you know how to extract data from pages, let’s see how to follow links from them. First thing is to extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup:12345&lt;ul class=&quot;pager&quot;&gt; &lt;li class=&quot;next&quot;&gt; &lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;&amp;rarr;&lt;/span&gt;&lt;/a&gt; &lt;/li&gt;&lt;/ul&gt; We can try extracting it in the shell:12&gt;&gt;&gt; response.css(&apos;li.next a&apos;).extract_first()&apos;&lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;→&lt;/span&gt;&lt;/a&gt;&apos; This gets the anchor element, but we want the attribute href. For that, Scrapy supports a CSS extension that let’s you select the attribute contents, like this:12&gt;&gt;&gt; response.css(&apos;li.next a::attr(href)&apos;).extract_first()&apos;/page/2/&apos; Let’s see now our spider modified to recursively follow the link to the next page, extracting data from it:123456789101112131415161718192021import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; start_urls = [ &apos;http://quotes.toscrape.com/page/1/&apos;, ] def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.css(&apos;small.author::text&apos;).extract_first(), &apos;tags&apos;: quote.css(&apos;div.tags a.tag::text&apos;).extract(), &#125; next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first() if next_page is not None: next_page = response.urljoin(next_page) yield scrapy.Request(next_page, callback=self.parse) Now, after extracting the data, the parse() method looks for the link to the next page, builds a full absolute URL using the urljoin() method (since the links can be relative) and yields a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages. What you see here is Scrapy’s mechanism(机制) of following links: when you yield a Request in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes. Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting. In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with pagination. A shortcut for creating RequestsAs a shortcut for creating Request objects you can use response.follow:1234567891011121314151617181920import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; start_urls = [ &apos;http://quotes.toscrape.com/page/1/&apos;, ] def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.css(&apos;span small::text&apos;).extract_first(), &apos;tags&apos;: quote.css(&apos;div.tags a.tag::text&apos;).extract(), &#125; next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first() if next_page is not None: yield response.follow(next_page, callback=self.parse) Unlike scrapy.Request, response.follow supports relative URLs directly - no need to call urljoin.(震惊,似乎挺厉害的) Note that response.follow just returns a Request instance; you still have to yield this Request. You can also pass a selector to response.follow instead of a string; this selector should extract necessary attributes:12for href in response.css(&apos;li.next a::attr(href)&apos;): yield response.follow(href, callback=self.parse) For elements there is a shortcut: response.follow uses their href attribute automatically. So the code can be shortened further:12for a in response.css(&apos;li.next a&apos;): yield response.follow(a, callback=self.parse) Noteresponse.follow(response.css(&#39;li.next a&#39;)) is not valid because response.css returns a list-like object with selectors for all results, not a single selector. A for loop like in the example above, or response.follow(response.css(&#39;li.next a&#39;)[0]) is fine. More examples and patternsHere is another spider that illustrates callbacks and following links, this time for scraping author information:12345678910111213141516171819202122import scrapyclass AuthorSpider(scrapy.Spider): name = &apos;author&apos; start_urls = [&apos;http://quotes.toscrape.com/&apos;] def parse(self, response): # follow links to author pages for href in response.css(&apos;.author + a::attr(href)&apos;): yield response.follow(href, self.parse_author) # follow pagination links for href in response.css(&apos;li.next a::attr(href)&apos;): yield response.follow(href, self.parse) def parse_author(self, response): def extract_with_css(query): return response.css(query).extract_first().strip() yield &#123; &apos;name&apos;: extract_with_css(&apos;h3.author-title::text&apos;), &apos;birthdate&apos;: extract_with_css(&apos;.author-born-date::text&apos;), &apos;bio&apos;: extract_with_css(&apos;.author-description::text&apos;), &#125; This spider will start from the main page, it will follow all the links to the authors pages calling the parse_author callback for each of them, and also the pagination links with the parse callback as we saw before. Here we’re passing callbacks to response.follow as positional arguments to make the code shorter; it also works for scrapy.Request. The parse_author callback defines a helper function to extract and cleanup the data from a CSS query and yields the Python dict with the author data. Another interesting thing this spider demonstrates(显示) is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured by the setting DUPEFILTER_CLASS.(不用担心死循环) Hopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy. As yet another example spider that leverages the mechanism of following links, check out the CrawlSpider class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it.(可以写自己的规则?是这个意思吧)Also, a common pattern is to build an item with data from more than one page, using a trick to pass additional data to the callbacks. Using spider argumentsYou can provide command line arguments to your spiders by using the -a option when running them:scrapy crawl quotes -o quotes-humor.json -a tag=humor(使用-a选项 添加attr)These arguments are passed to the Spider’s __init__ method and become spider attributes by default. In this example, the value provided for the tag argument will be available via self.tag. You can use this to make your spider fetch only quotes with a specific tag, building the URL based on the argument:1234567891011121314151617181920import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; def start_requests(self): url = &apos;http://quotes.toscrape.com/&apos; tag = getattr(self, &apos;tag&apos;, None) if tag is not None: url = url + &apos;tag/&apos; + tag yield scrapy.Request(url, self.parse) def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.css(&apos;small.author::text&apos;).extract_first(), &#125;(先处理得到的文本) next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first() (再去获取下一个page) if next_page is not None: yield response.follow(next_page, self.parse) If you pass the tag=humor argument to this spider, you’ll notice that it will only visit URLs from the humor tag, such as http://quotes.toscrape.com/tag/humor. You can learn more about handling spider arguments here.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy Pass 2]]></title>
    <url>%2F2018%2F08%2F16%2Fscrapy2%2F</url>
    <content type="text"><![CDATA[Learn Scrapy pass 2安装(虚拟环境)Python packages can be installed either globally (a.k.a system wide), or in user-space. We do not recommend installing scrapy system wide.(并不推荐把python package安装成系统的一部分) Instead, we recommend that you install scrapy within a so-called “virtual environment” (virtualenv). Virtualenvs allow you to not conflict with already-installed Python system packages (which could break some of your system tools and scripts), and still install packages normally with pip (without sudo and the likes).(不会对系统现有的包产生冲突,可以使用普通用户的形式进行安装) Once you have created a virtualenv, you can install scrapy inside it with pip, just like any other Python package.(这里不介绍安装env,提一下,pycharm控制台自带虚拟环境) Python virtualenvs can be created to use Python 2 by default, or Python 3 by default.If you want to install scrapy with Python 3, install scrapy within a Python 3 virtualenv.And if you want to install scrapy with Python 2, install scrapy within a Python 2 virtualenv. Scrapy 使用示例We are going to scrape quotes.toscrape.com, a website that lists quotes from famous authors. This tutorial will walk you through these tasks: Creating a new Scrapy project Writing a spider to crawl a site and extract data Exporting the scraped data using the command line Changing spider to recursively follow links Using spider arguments Creating a projectBefore you start scraping, you will have to set up a new Scrapy project. Enter a directory where you’d like to store your code and run:scrapy startproject tutorialThis will create a tutorial directory with the following contents:1234567891011121314151617得到默认的初始化目录tutorial/ scrapy.cfg # deploy configuration file 配置文件 tutorial/ # project&apos;s Python module, you&apos;ll import your code from here __init__.py items.py # project items definition file middlewares.py # project middlewares file pipelines.py # project pipelines file settings.py # project settings file 配置文件 spiders/ # a directory where you&apos;ll later put your spiders __init__.py Our first SpiderSpiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass scrapy.Spider and define the initial requests to make, optionally how to follow links in the pages, and how to parse the downloaded page content to extract data.(Spiders目录是自己定义的,通过爬虫来爬取website)This is the code for our first Spider. Save it in a file named quotes_spider.py under the (注意位置) tutorial/spidersdirectory in your project:12345678910111213141516171819import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; def start_requests(self): urls = [ &apos;http://quotes.toscrape.com/page/1/&apos;, &apos;http://quotes.toscrape.com/page/2/&apos;, ] for url in urls: yield scrapy.Request(url=url, callback=self.parse) def parse(self, response): page = response.url.split(&quot;/&quot;)[-2] filename = &apos;quotes-%s.html&apos; % page with open(filename, &apos;wb&apos;) as f: f.write(response.body) self.log(&apos;Saved file %s&apos; % filename) 一个带有 yield 的函数就是一个 generator，它和普通函数不同，生成一个 generator 看起来像函数调用，但不会执行任何函数代码，直到对其调用 next()（在 for 循环中会自动调用 next()）才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个 yield 语句就会中断，并返回一个迭代值，下次执行时从 yield 的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被 yield 中断了数次，每次中断都会通过 yield 返回当前的迭代值。 yield 的好处是显而易见的，把一个函数改写为一个 generator 就获得了迭代能力，比起用类的实例保存状态来计算下一个 next() 的值，不仅代码简洁，而且执行流程异常清晰。As you can see, our Spider subclasses scrapy.Spider and defines some attributes and methods: name: identifies the Spider.(定义爬虫名字) It must be unique within a project, that is, you can’t set the same name for different Spiders. start_requests(): must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests. parse(): a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of TextResponse that holds the page content and has further helpful methods to handle it. The parse() method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (Request) from them.(提取信息并得到下一步url) How to run our spiderTo put our spider to work, go to the project’s top level directory and run:scrapy crawl quotes(quotes就是之前定义的name)This command runs the spider with name quotes that we’ve just added, that will send some requests for the quotes.toscrape.com domain. You will get an output similar to this:12345678910... (omitted for brevity)2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:60232016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET http://quotes.toscrape.com/robots.txt&gt; (referer: None)2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/1/&gt; (referer: None)2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/2/&gt; (referer: None)2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished) Now, check the files in the current directory. You should notice that two new files have been created: quotes-1.html and quotes-2.html, with the content for the respective URLs, as our parse method instructs. What just happened under the hood?Scrapy schedules the scrapy.Request objects returned by the start_requests method of the Spider. Upon receiving a response for each one, it instantiates Response objects and calls the callback method associated with the request (in this case, the parse method) passing the response as argument.(通过the start_requests method of the Spider将request的请求发出去,得到response,然后对于每个response调用callback) A shortcut to the start_requests methodInstead of implementing(执行) a start_requests() method that generates scrapy.Request objects from URLs, you can just define a start_urls class attribute with a list of URLs. This list will then be used by the default implementation of start_requests() to create the initial requests for your spider:(这个start_urls会自动调用默认的方法去生成初始的requests)123456789101112131415import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; start_urls = [ &apos;http://quotes.toscrape.com/page/1/&apos;, &apos;http://quotes.toscrape.com/page/2/&apos;, ] def parse(self, response): page = response.url.split(&quot;/&quot;)[-2] filename = &apos;quotes-%s.html&apos; % page with open(filename, &apos;wb&apos;) as f: f.write(response.body) The parse() method will be called to handle each of the requests for those URLs, even though we haven’t explicitly told Scrapy to do so. This happens because parse() is Scrapy’s default callback method, which is called for requests without an explicitly assigned callback.(默认会处理response,无需明显调用) Extracting dataThe best way to learn how to extract data with Scrapy is trying selectors using the shell (Scrapy shell). Run:scrapy shell &#39;http://quotes.toscrape.com/page/1/&#39;`You will see something like:123456789101112131415[ ... Scrapy log here ... ]2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/1/&gt; (referer: None)[s] Available Scrapy objects:[s] scrapy scrapy module (contains scrapy.Request, scrapy.Selector, etc)[s] crawler &lt;scrapy.crawler.Crawler object at 0x7fa91d888c90&gt;[s] item &#123;&#125;[s] request &lt;GET http://quotes.toscrape.com/page/1/&gt;[s] response &lt;200 http://quotes.toscrape.com/page/1/&gt;[s] settings &lt;scrapy.settings.Settings object at 0x7fa91d888c10&gt;[s] spider &lt;DefaultSpider &apos;default&apos; at 0x7fa91c8af990&gt;[s] Useful shortcuts:[s] shelp() Shell help (print this help)[s] fetch(req_or_url) Fetch request (or URL) and update local objects[s] view(response) View response in a browser&gt;&gt;&gt; Using the shell, you can try selecting elements using CSS with the response object:12&gt;&gt;&gt; response.css(&apos;title&apos;)[&lt;Selector xpath=&apos;descendant-or-self::title&apos; data=&apos;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&apos;&gt;] (弱弱的说一句,有点像选择器啊)The result of running response.css(&#39;title&#39;) is a list-like object called SelectorList, which represents a list of Selector objects that wrap around XML/HTML elements and allow you to run further queries to fine-grain the selection or extract the data.(果然)To extract the text from the title above, you can do:12&gt;&gt;&gt; response.css(&apos;title::text&apos;).extract()[&apos;Quotes to Scrape&apos;] There are two things to note here:one is that we’ve added ::text to the CSS query, to mean we want to select only the text elements directly inside element. If we don’t specify ::text, we’d get the full title element, including its tags:12&gt;&gt;&gt; response.css(&apos;title&apos;).extract()[&apos;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&apos;] The other thing is that the result of calling .extract() is a list, because we’re dealing with an instance of SelectorList. When you know you just want the first result, as in this case, you can do:12&gt;&gt;&gt; response.css(&apos;title::text&apos;).extract_first()&apos;Quotes to Scrape&apos; As an alternative, you could’ve written:12&gt;&gt;&gt; response.css(&apos;title::text&apos;)[0].extract()&apos;Quotes to Scrape&apos; However, using .extract_first() avoids an IndexError and returns None when it doesn’t find any element matching the selection. There’s a lesson here: for most scraping code, you want it to be resilient to errors due to things not being found on a page, so that even if some parts fail to be scraped, you can at least get some data.(即使你没有得到你想要的东西,你也可以得到一些data) Besides the extract() and extract_first() methods, you can also use the re() method to extract using regular expressions:123456&gt;&gt;&gt; response.css(&apos;title::text&apos;).re(r&apos;Quotes.*&apos;)[&apos;Quotes to Scrape&apos;]&gt;&gt;&gt; response.css(&apos;title::text&apos;).re(r&apos;Q\w+&apos;)[&apos;Quotes&apos;]&gt;&gt;&gt; response.css(&apos;title::text&apos;).re(r&apos;(\w+) to (\w+)&apos;)[&apos;Quotes&apos;, &apos;Scrape&apos;] In order to find the proper CSS selectors to use, you might find useful opening the response page from the shell in your web browser using view(response). You can use your browser developer tools or extensions like Firebug (see sections about Using Firebug for scraping and Using Firefox for scraping). Selector Gadget is also a nice tool to quickly find CSS selector for visually selected elements, which works in many browsers. 不止可以使用css选择器,还可以使用xpath1234&gt;&gt;&gt; response.xpath(&apos;//title&apos;)[&lt;Selector xpath=&apos;//title&apos; data=&apos;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&apos;&gt;]&gt;&gt;&gt; response.xpath(&apos;//title/text()&apos;).extract_first()&apos;Quotes to Scrape&apos; XPath expressions are very powerful, and are the foundation of Scrapy Selectors. In fact, CSS selectors are converted to XPath under-the-hood. You can see that if you read closely the text representation of the selector objects in the shell.(没想到啊,竟然是这样) we encourage you to learn XPath even if you already know how to construct CSS selectors, it will make scraping much easier.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy Pass 1]]></title>
    <url>%2F2018%2F08%2F16%2Fscrapy1%2F</url>
    <content type="text"><![CDATA[Learn Scrapy pass 1Walk-through of an example spider1234567891011121314151617import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; start_urls = [ &apos;http://quotes.toscrape.com/tag/humor/&apos;, ] def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.xpath(&apos;span/small/text()&apos;).extract_first(), &#125; next_page = response.css(&apos;li.next a::attr(&quot;href&quot;)&apos;).extract_first() if next_page is not None: yield response.follow(next_page, self.parse) Put this in a text file, name it to something like quotes_spider.py and run the spider using the runspider command:1scrapy runspider quotes_spider.py -o quotes.json When this finishes you will have in the quotes.json file a list of the quotes in JSON format, containing text and author, looking like this (reformatted here for better readability):12345678910111213[&#123; &quot;author&quot;: &quot;Jane Austen&quot;, &quot;text&quot;: &quot;\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d&quot;&#125;,&#123; &quot;author&quot;: &quot;Groucho Marx&quot;, &quot;text&quot;: &quot;\u201cOutside of a dog, a book is man&apos;s best friend. Inside of a dog it&apos;s too dark to read.\u201d&quot;&#125;,&#123; &quot;author&quot;: &quot;Steve Martin&quot;, &quot;text&quot;: &quot;\u201cA day without sunshine is like, you know, night.\u201d&quot;&#125;,...] When you ran the command scrapy runspider quotes_spider.py, Scrapy looked for a Spider definition inside it and ran it through its crawler engine.(在爬取引擎中运行爬虫) The crawl started by making requests to the URLs defined in the start_urls attribute (in this case, only the URL for quotes in humor category)(发送请求给start_urls) and called the default callback method parse, passing the response object as an argument.(回调函数处理response) In the parse callback, we loop through the quote elements using a CSS Selector (通过使用css选择器), yield a Python dict with the extracted quote text and author(使用python字典去提取text文本与作者) , look for a link to the next page and schedule another request using the same parse method as callback.(找到下一个page,调用相同的parse) Here you notice one of the main advantages about Scrapy: requests are scheduled and processed asynchronously. This means that Scrapy doesn’t need to wait for a request to be finished and processed, it can send another request or do other things in the meantime. This also means that other requests can keep going even if some request fails or an error happens while handling it.(处理异步,请求储存在调度器里面,不知道是不是队列形式) While this enables you to do very fast crawls (sending multiple concurrent requests at the same time, in a fault-tolerant way) Scrapy also gives you control over the politeness of the crawl through a few settings. You can do things like setting a download delay between each request, limiting amount of concurrent requests per domain or per IP, and even using an auto-throttling extension that tries to figure out these automatically.(可控制性:可以控制scrapy每个域名,ip访问的数量等) This is using feed exports to generate the JSON file, you can easily change the export format (XML or CSV, for example) or the storage backend (FTP or Amazon S3, for example). You can also write an item pipeline to store the items in a database.(输出形式可以多样) 还有等等等等的特性,先不了解(看了也不懂),之后学了再看看]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bumblebee控制独显]]></title>
    <url>%2F2018%2F08%2F15%2FBumblebee%E6%8E%A7%E5%88%B6%E7%8B%AC%E6%98%BE%2F</url>
    <content type="text"><![CDATA[Bumblebee使用控制独显安装:bumblebee - 提供守护进程以及程序的主要安装包。mesa - 开源的 OpenGL 标准实现。对于合适的NVIDIA驱动。xf86-video-intel - Intel 驱动（可选）。对于32位程序 (必须启用Multilib）在64位机器上的支持，安装: lib32-virtualgl - 为32位应用提供的渲染/显示桥。lib32-nvidia-utils 或者 lib32-nvidia-340xx-utils（和64位对应）。要使用Bumblebee，请确保添加你的用户到 bumblebee 组：gpasswd -a user bumblebee并启用(enable) bumblebeed.service。之后重启系统 可以明显的感受到风扇转速下降 重启之后测试安装 mesa-demos并使用 glxgears 测试 Bumblebee 是否工作：optirun glxgears -info看到有图形出现,并且风扇开始转动,独显开始工作,成功!!]]></content>
      <categories>
        <category>arch</category>
      </categories>
      <tags>
        <tag>Nvidia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-15 plane war日志]]></title>
    <url>%2F2018%2F08%2F15%2F2018-08-15%2F</url>
    <content type="text"><![CDATA[小游戏制作完成算是一个类似躲避球的游戏吧..花了两三天时间(大部分在上个学期完成了这次算是重新构建了一下,变了一下形式总的来说还可以接受吧]]></content>
      <categories>
        <category>开发日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-14 plane war日志]]></title>
    <url>%2F2018%2F08%2F14%2F2018-08-14%2F</url>
    <content type="text"><![CDATA[基本构建完成判断碰撞的逻辑没有作用,需要改进]]></content>
      <categories>
        <category>开发日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java_swing 绘制图形]]></title>
    <url>%2F2018%2F08%2F12%2Fjava-swing%2F</url>
    <content type="text"><![CDATA[在java的jpanel绘制图形使用paint方法,在里面使用super.paint清除之前的残留在while里使用repaint进行循环绘画12345678public void display() &#123; this.repaint(); &#125; public void paint(Graphics g) &#123; super.paint(g); dao.drawBackground(g); dao.drawState(g, hero); &#125; 123456789101112while(true) &#123; start.display(); try &#123; Thread.sleep(300); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;]]></content>
      <categories>
        <category>swing</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下/srv、/var和/tmp的职责区分]]></title>
    <url>%2F2018%2F08%2F12%2F%E5%88%86%E5%8C%BA%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[(记录备忘)关于linux下/srv、/var和/tmp的职责区分 (转载自这儿)/srv ：主要用来存储本机或本服务器提供的服务或数据。（用户主动生产的数据、对外提供服务） /srv contains site-specific data which is served by this system. /var ：系统产生的不可自动销毁的缓存文件、日志记录。（系统和程序运行后产生的数据、不对外提供服务、只能用户手动清理）（包括mail、数据库文件、日志文件） /var contains variable data files. This includes spool directories and files, administrative and logging data, and transient and temporary files.Some portions of /var are not shareable between different systems. For instance, /var/log, /var/lock, and /var/run. Other portions may be shared, notably /var/mail, /var/cache/man, /var/cache/fonts, and /var/spool/news./var is specified here in order to make it possible to mount /usr read-only. Everything that once went into /usr that is written to during system operation (as opposed to installation and software maintenance) must be in /var.If /var cannot be made a separate partition, it is often preferable to move /var out of the root partition and into the /usr partition. (This is sometimes done to reduce the size of the root partition or when space runs low in the root partition.) However, /var must not be linked to /usr because this makes separation of /usr and /var more difficult and is likely to create a naming conflict. Instead, link /var to /usr/var.Applications must generally not add directories to the top level of /var. Such directories should only be added if they have some system-wide implication, and in consultation with the FHS mailing list. /tmp ：保存在使用完毕后可随时销毁的缓存文件。（有可能是由系统或程序产生、也有可能是用户主动放入的临时数据、系统会自动清理） The /tmp directory must be made available for programs that require temporary files.Programs must not assume that any files or directories in /tmp are preserved between invocations of the program. 所以，服务器被用作Web开发时，html文件更应该被放在/srv/www下，而不是/var/www下（因为/srv目录是新标准中才有的，出现较晚；而且Apache将/var/www设为了web默认目录，所以现在绝大多数人都把web文件放在/var/www，这是个历史遗留问题）。 如ftp、流媒体服务等也应该被放在/srv对应的目录下。如果对应目录太大，应该另外挂载分区。]]></content>
      <categories>
        <category>arch</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-12 java plane war记录日志]]></title>
    <url>%2F2018%2F08%2F12%2F2018-08-12%2F</url>
    <content type="text"><![CDATA[之前旧的游戏没有完成,里面用的是timer.schdule本次重新写打算用thread现在还没有想好使用什么方法写子弹和敌人(是不是上一次一样使用list)linux下似乎swing的绘制有点卡,不知道是什么问题…. 还有就是遇到了linux下workbanch闪退的问题解决方法是: rm -rf .mysql/workbench/]]></content>
      <categories>
        <category>开发日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[备份arch系统]]></title>
    <url>%2F2018%2F08%2F10%2F%E5%A4%87%E4%BB%BDarch%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[使用rsync进行备份rsync的六种模式123456rsync [OPTION]... SRC DESTrsync [OPTION]... SRC [USER@]host:DESTrsync [OPTION]... [USER@]HOST:SRC DESTrsync [OPTION]... [USER@]HOST::SRC DESTrsync [OPTION]... SRC [USER@]HOST::DESTrsync [OPTION]... rsync://[USER@]HOST[:PORT]/SRC [DEST] 本地备份的话我们使用第一种 大佬使用的备份脚本,通过指定/的目录来进行备份123456789101112131415161718192021222324252627#!/bin/zsh -e#进入主目录cd $(dirname $0)#判断参数个数时候小于2或者大于3(也就是可以是2个参数或者是3个参数)if [[ $# -lt 2 || $# -gt 3 ]]; then echo &quot;usage: $0 SRC_DIR DEST_DIR [-w]&quot; exit 1fi#得到参数src=$1dest=$2doit=$3if [[ $doit == -w ]]; then dry=else dry=&apos;-n&apos;firsync --archive --one-file-system --inplace --hard-links \ --human-readable --numeric-ids --delete --delete-excluded \ --acls --xattrs --sparse \ --itemize-changes --verbose --progress \ --exclude=&apos;*~&apos; --exclude=__pycache__ \ --exclude-from=root.exclude \ $src $dest $dry 以下列出rsync参数,使用到的将使用加粗列出 -v, –verbose 详细模式输出。-i, –itemize-changes 列出改变(不知道是不是这个意思)-q, –quiet 精简输出模式。-c, –checksum 打开校验开关，强制对文件传输进行校验。-a, –archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD。-A –acls This option causes rsync to update the destination ACLs to be the same as the source ACLs. The option also implies –perms.-X –xattrs This option causes rsync to update the destination extended attributes to be the same as the source ones.(上面两个选项保留文件 ACL 和扩展属性)-r, –recursive 对子目录以递归模式处理。-R, –relative 使用相对路径信息。-b, –backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用–suffix选项来指定不同的备份文件前缀。–backup-dir 将备份文件(如~filename)存放在在目录下。-suffix=SUFFIX 定义备份文件前缀。-u, –update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件，不覆盖更新的文件。–inplace 更新目标文件,RSYNC将写入的更新数据直接写入目标文件。-l, –links 保留软链结。-L, –copy-links 想对待常规文件一样处理软链结。–copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结。–safe-links 忽略指向SRC路径目录树以外的链结。-H, –hard-links 保留硬链结。-p, –perms 保持文件权限。就是 –acls-o, –owner 保持文件属主信息。-g, –group 保持文件属组信息。-D, –devices 保持设备文件信息。-t, –times 保持文件时间信息。-S, –sparse 对稀疏文件进行特殊处理以节省DST的空间。-n, –dry-run 进行try的尝试,不做改变。-w, –whole-file 拷贝文件，不进行增量检测。-x, –one-file-system 不要跨越文件系统边界。-B, –block-size=SIZE 检验算法使用的块尺寸，默认是700字节。-e, –rsh=command 指定使用rsh、ssh方式进行数据同步。–rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息。-C, –cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件。–existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件。–delete 删除那些DST中SRC没有的文件。–delete-excluded 同样删除接收端那些被该选项指定排除的文件。–delete-after 传输结束以后再删除。–ignore-errors 及时出现IO错误也进行删除。–max-delete=NUM 最多删除NUM个文件。–partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输。–force 强制删除目录，即使不为空。–numeric-ids 将数字的用户和组id匹配为用户名和组名。–timeout=time ip超时时间，单位为秒。-I, –ignore-times 不跳过那些有同样的时间和长度的文件。–size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间。–modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0。-T –temp-dir=DIR 在DIR中创建临时文件。–compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份。-P 等同于 –partial。–progress 显示备份过程。-z, –compress 对备份的文件在传输时进行压缩处理。–include=PATTERN 指定不排除而需要传输的文件模式。–exclude-from=FILE 排除FILE中指定模式的文件。–include-from=FILE 不排除FILE指定模式匹配的文件。–version 打印版本信息。–address 绑定到特定的地址。–config=FILE 指定其他的配置文件，不使用默认的rsyncd.conf文件。–port=PORT 指定其他的rsync服务端口。–blocking-io 对远程shell使用阻塞IO。-stats 给出某些文件的传输状态。–progress 在传输时现实传输过程。–log-format=formAT 指定日志文件格式。–password-file=FILE 从FILE中得到密码。–bwlimit=KBPS 限制I/O带宽，KBytes per second。-h, –help 显示帮助信息。–human-readable 以更可读的格式输出数字。 看了好多教程rsync+btrfs单纯使用rsync..发现还是单纯的使用rsync比较省事 排除的目录12345/media/*/sys/*/proc/*/mnt/*/tmp/* 但 @依云仙子 所说 --one-file-system 已经排除了特殊的目录 目前就先这样备份了123456sudo rsync --archive --one-file-system --hard-links \ --acls --xattrs --sparse \ --human-readable --numeric-ids --delete --delete-excluded \ --itemize-changes --verbose --progress \ --exclude-from=backup.exclude \ / /run/media/s/5ff9a14c-20ba-41c5-8b7e-f6b095326200/Backup/backup (讲道理应该可以) 再把代码备份到github上就差不多了(溜了)]]></content>
      <categories>
        <category>rsync</category>
      </categories>
      <tags>
        <tag>备份</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-10 备份日志]]></title>
    <url>%2F2018%2F08%2F10%2F2018-08-10%2F</url>
    <content type="text"><![CDATA[看了好多教程,还是不懂与其这样不如使用最简单的备份操作进行备份(借口,就是懒)还是得学习啊,感觉神码也不懂..]]></content>
      <categories>
        <category>arch备份日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-09 备份日志]]></title>
    <url>%2F2018%2F08%2F09%2F2018-08-09-1%2F</url>
    <content type="text"><![CDATA[备份准备工作已开始分区已经完成,也已经加密还需要详细了解rsync,增量式备份由于属于本地备份,或许未能完全了解rsync个人想法 网上全都是简单的参数缩写,还是得查man page,记倒不是问题ps:十分害怕硬盘gg,听说希捷的容易坏0-0]]></content>
      <categories>
        <category>arch备份日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-09 lfs编译日志]]></title>
    <url>%2F2018%2F08%2F09%2F2018-08-09%2F</url>
    <content type="text"><![CDATA[已经完成lfs最小的构建偷偷的懒散的躺了好十几天..愧疚(逃开始下一个任务==&gt;备份系统]]></content>
      <categories>
        <category>lfs编译日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux For Scratch Pass 6]]></title>
    <url>%2F2018%2F08%2F09%2Flfs6%2F</url>
    <content type="text"><![CDATA[让 LFS 系统可引导创建 /etc/fstab 文件/etc/fstab 文件的作用是让其它程序确定存储设备的默认挂载点、挂载参数和检查信息（例如完整性检测）。12345678910111213141516cat &gt; /etc/fstab &lt;&lt; &quot;EOF&quot;# Begin /etc/fstab# file system mount-point type options dump fsck# order/dev/&lt;xxx&gt; / &lt;fff&gt; defaults 1 1/dev/&lt;yyy&gt; swap swap pri=1 0 0proc /proc proc nosuid,noexec,nodev 0 0sysfs /sys sysfs nosuid,noexec,nodev 0 0devpts /dev/pts devpts gid=5,mode=620 0 0tmpfs /run tmpfs defaults 0 0devtmpfs /dev devtmpfs mode=0755,nosuid 0 0# End /etc/fstabEOF 其中，， 和 请使用适当的值替换。例如 sda2，sda5 和 ext4。关于文件中六个字段的含义，请查看 man 5 fstab（译者注：fsck 列的数值来决定需要检查的文件系统的检查顺序。允许的数字是0, 1, 和2。根目录应当获得最高的优先权 1, 其它所有需要被检查的设备设置为 2。0 表示设备不会被 fsck 所检查）。 基于 MS-DOS 或者是来源于 Windows 的文件系统（例如：vfat，ntfs，smbfs，cifs，iso9660，udf）需要在挂载选项中添加「iocharset」，才能让非 ASCII 字符的文件名正确解析。此选项的值应该与语言区域设置的值相同，以便让内核能正确处理。此选项在相关字符集定义已为内核内建或是编译为模块时生效（在文件系统 -&gt; 本地语言支持中查看）。此外，vfat 和 smbfs 还需启用「codepage」支持。例如，想要挂载 USB 闪存设备，zh-CN.GBK 用户需要在 /etc/fstab 中添加以下的挂载选项：noauto,user,quiet,showexec,iocharset=koi8r,codepage=866对于 zh_CN.UTF-8 用户的对应选项是：noauto,user,quiet,showexec,iocharset=utf8,codepage=936主要是设置挂载参数本机12345678910$ cat /etc/fstab# Static information about the filesystems.# See fstab(5) for details.# &lt;file system&gt; &lt;dir&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt;# /dev/sda5UUID=c36eedfb-08b3-4e28-8483-03f6d5f1ad0c / ext4 rw,relatime,data=ordered 0 1# /dev/sda2UUID=608A-F457 /boot vfat rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=iso8859-1,shortname=mixed,utf8,errors=remount-ro 0 0 之后就是1.编译内核2.安装grub引导(注意备份)3.重新启动 之后安装其他的小程序 sudo,dhcp,wget等 结束]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-07-25 lfs编译日志]]></title>
    <url>%2F2018%2F07%2F25%2F2018-07-25%2F</url>
    <content type="text"><![CDATA[lfs编译告一段落系统大半构建完成]]></content>
      <categories>
        <category>lfs编译日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux For Scratch Pass 5]]></title>
    <url>%2F2018%2F07%2F25%2Flfs5%2F</url>
    <content type="text"><![CDATA[System V is the classic boot process that has been used in Unix and Unix-like systems such as Linux since about 1983.It consists of a small program, init, that sets up basic programs such as login (via getty) and runs a script. This script, usually named rc, controls the execution of a set of additional scripts that perform the tasks required to initialize the system. The init program is controlled by the /etc/inittab file and is organized into run levels that can be run by the user: 123456780 — halt1 — Single user mode(单用户模式)2 — Multiuser, without networking3 — Full multiuser mode4 — User definable5 — Full multiuser mode with display manager6 — rebootThe usual default run level is 3 or 5. 安装LFS-Bootscripts该软件包里面有一系列系统启动和关机的脚本 概述传统的 Linux 不管硬件是否真实存在，都以创建静态设备的方法来处理硬件，因此需要在 /dev 目录下创建大量的设备节点文件(有时会有上千个)。这通常由 MAKEDEV 脚本完成，它通过大量调用 mknod 程序 为这个世界上可能存在的每一个设备建立对应的主设备号和次设备号。 而使用 udev方法，只有当内核检测到硬件接入，才 会建立对应的节点文件。因为需要在系统启动的时候重新建立设备节点文件(动态建立文件)，所以将它存储在 devtmpfs 文件系统中（完全存在于内存中的虚拟文件系统）。设备节点文件无需太多的空间，所以占用的内存也很小。(动态建立) 历史2000 年 2 月，一种名叫 devfs 的文件系统被合并到 2.3.46 内核版本，2.4 系列的稳定内核中基本可用。尽管它存在于内核的源代码中，但是这种动态创建设备的方法却从来都得不到核心内核开发者的大力支持。 问题存在于它处理设备的检测、创建和命令的方式，其中最大的问题莫过于它对设备节点的命名方式。大部分开发者的观点是，设备的命名应该由系统的所有者决定，而不是开发者。 devfs 存在一个严重的设计缺陷：它存在严重的 race conditions 问题(有两个方面的问题,两个不相干的进程争取一个相同资源,两个相干的进程竞争同一资源,相互等待,命名的方式会使得目标不明确)，如不对内核做大量的修改就无法修正这一问题。最终，因为缺乏有效的维护，在 2006 年 6 月终被移出内核源代码。再后来，随着非稳定的 2.5 版本的内核开发，到稳定的 2.6 内核，又出现了一种全新的虚拟文件系统 sysfs。sysfs 的工作是将系统的硬件配置导入到用户空间进程。通过对用户空间可视化的改善，代替devfs变得更加现实。 Sysfs上文简单的提及了 sysfs 文件系统。有些人可能会问，sysfs 到底是如何知道当前系统有哪些设备，这些设备又该使用什么设备号呢。对于那些已经编译进内核的设备，会在内核检测到时直接注册为 sysfs 对象（devtmpfs 内建）(通过保存在内存的设备系统,来注册设备)。对于编译为内核模块的设备，将会在模块载入的时候注册。一旦 sysfs 文件系统挂载到 /sys，已经在 sysfs 注册的硬件数据就可以被用户空间的进程使用， udevd也就能够处理了（包括对设备节点进行修改）。(总的来说,是先通过devtmpfs来注册电脑上有的设备,动态注册设备,之后一旦 sysfs 文件系统挂载到 /sys,就将这些注册的设备给用户进程使用) 设备节点的创建设备文件是通过内核中的 devtmpfs 文件系统创建的。任何想要注册的设备都需要通过 devtmpfs （通过驱动程序核心）实现。每当一个devtmpfs实例挂载到 /dev，就 会建立一个设备节点文件，它拥有固定 的名称、权限、所有者。很短的时间之后，内核将给 udevd 一个 uevent。基于 /etc/udev/rules.d，/lib/udev/rules.d 和 /run/udev/rules.d目录内文件指定的规则，udevd 将会建立到设备节点文件的额外符号链接，这有可能更改其权限，所有者，所在组，或者是更改 udevd 内建接口（名称）。(创建符号链接的时候可能会出现问题)这三个目录下的规则都像 LFS-Bootscripts 包那样标有数字，所有三个目录都会统一到一起。如果 udevd 找不到 和所创建设备相应的规则，它会保留 devtmpfs 里初始化时使用过的权限和属主。 加载模块编译成模块的设备驱动可能会包含别名。别名可以通过modinfo打印出来查看，一般是模块支持的特定总线的设备描述符。举个例子，驱动snd-fm801支持厂商 ID 为 0x1319 以及设备 ID 为 0x0801 的设备，它包含一个 “pci:v00001319d00000801sv*sd*bc04sc01i*” (vesion,device,sv*sd*bc04sc01i*)的别名，总线驱动导出该驱动别名并通过 sysfs 处理相关设备。例如，文件 /sys/bus/pci/devices/0000:00:0d.0/modalias 应该会包含字符串 “pci:v00001319d00000801sv00001319sd00001319bc04sc01i00”。Udev 采用的默认规则会让 udevd 根据 uevent 环境变量 MODALIAS 的内容（它应该和 sysfs 里的 modalias 文件内容一样）调用 /sbin/modprobe，这样就可以加载在通配符扩展后能和这个字符串一致的所有模块。(modprobe命令 用于智能地向内核中加载模块或者从内核中移除模块。)在这个例子里，意味着，除了 snd-fm801 之外，一个已经废弃的（不是我们所希望的）驱动 forte 如果存在的话也会被加载。下面有几种可以避免加载多余驱动的方式。 内核本身也能够根据需要加载网络协议，文件系统以及 NLS 支持模块。 处理热插拔/动态设备在你插入一个设备时，例如一个通用串行总线（USB）MP3 播放器，内核检测到设备已连接就会生成一个uevent。这个 uevent 随后会被上面所说的udevd处理。 加载模块和创建设备时可能碰到的问题在自动创建设备节点时可能会碰到一些问题。 内核模块没有自动加载Udev 只会加载包含有特定总线别名而且已经被总线驱动导出到 sysfs 下的模块。在其它情况下，你应该考虑用其它方式加载模块。采用 Linux-4.15.3，Udev 可以加载编写合适的 INPUT、IDE、PCI、USB、SCSI、SERIO 和 FireWire设备驱动。 要确定你希望加载的驱动是否支持 Udev，可以用模块名字作为参数运行 modinfo。然后查看/sys/bus下的设备目录里是否有个modalias文件。 如果在 sysfs 下能找到modalias文件，那么就能驱动这个设备并可以直接操作它，但是如果该文件里没有包含设备别名，那意味着这个驱动有问题。我们可以先尝试不依靠 Udev直接加载驱动，等这个问题以后解决。 如果在 /sys/bus 下的相应目录里没有 modalias 的话，意味着内核开发人员还没有为这个总线类型增加 modalias 支持。使用 Linux-4.15.3 内核，应该是 ISA 总线的问题。希望这个可以在后面的内核版本里得到解决。 Udev 不会尝试加载类似 snd-pcm-oss 这样的“封装”驱动，也不会加载类似 loop 这样的非硬件驱动。(loop是指循环?) 内核驱动没有自动加载，Udev 也没有尝试加载如果是 “封装” 模块只是强化其它模块的功能（比如，snd-pcm-oss 模块通过允许 OSS 应用直接访问声卡的方式加强了 snd-pcm 模块的功能），需要配置 modprobe 在 Udev 加载硬件驱动模块后再加载相应的封装模块。可以在任意 /etc/modprobe.d/&lt;filename&gt;.conf文件里增加一行 “softdep”，例如：softdep snd-pcm post: snd-pcm-oss(设置加载模块方式)请注意 “softdep” 也支持 pre: 的依赖方式，或者混合 pre: 和 post:。查看 modprobe.d(5) 手册了解更多关于 “softdep” 语法和功能的信息。 如果问题模块不是一个封装而且也是有用的，配置 modules开机脚本在引导系统的时候加载模块。这样需要把模块名字添加到 /etc/sysconfig/modules 文件里的单独一行。这个也可以用于封装模块，但是只是备用方式。 Udev 加载了一些无用模块要么不要编译该模块，或者把它加入到模块黑名单 /etc/modprobe.d/blacklist.conf 里，像下面的例子里屏蔽了 forte 模块：blacklist forte 被屏蔽的模块仍然可以用 modprobe 强行加载。 Udev 创建了错误的设备节点，或错误的软链接这个情况通常是因为设备匹配错误。例如，一条写的不好的规则可能同时匹配到 SCSI 磁盘（希望加载的）和对应厂商的 SCSI 通用设备（错误的）。找出这条问题规则，并通过 udevadm info 命令的帮助改得更具体一些。(一条规则对应到两个设备) Udev 规则工作不可靠这可能是上个问题的另一种表现形式。如果不是，而且你的规则使用了 sysfs 特性，那可能是内核时序问题，希望在后面版本内核里能解决。目前的话，你可以暂时建立一条规则等待使用的 sysfs 特性，并附加到/etc/udev/rules.d/10-wait_for_sysfs.rules 文件里（如果没有这个文件就创建一个）。如果你碰到这种情形请通知 LFS 开发邮件列表，这个对我们有帮助。 Udev 没有创建设备后面的内容会假设驱动已经静态编译进内核或已经作为模块加载，而且你也已经确认 Udev 没有创建相应的设备节点。如果内核驱动没有将自己的数据导出到 sysfs 里，Udev 就没有相关信息来创建设备节点。这通常发生在内核树之外的第三方驱动里。我们可以使用合适的主/副设备数字 ID（查看内核文档里或第三方驱动厂商提供的文档里的 devices.txt 文件） 在 /lib/udev/devices 目录里创建一个静态设备节点。这个静态设备节点随后会被 udev 引导脚本复制到 /dev 里。 设备名称顺序在重启后随机改变这是因为 Udev被设计成并行处理 uevents 并加载模块，所以是不可预期的顺序。这个不会“修复”。你不应该依赖稳定的内核模块名称。而是，在检测到设备的稳定特征，比如序列号或 Udev 安装的一些*_id应用的输出，来判断设备的稳定名称，之后 创建自己的规则生成相应的软链接。 管理设备网络设备Udev, by default, names network devices according to Firmware/BIOS data or physical characteristics like the bus, slot, or MAC address. (通过固件/bios或者说是设备自带的属性进行命名)这种方式使得命名一致,而不是基于发现网卡的时间来确定(老方法) 新的方法一般会变成enp5s0 or wlp3s0这样的形式你也可以禁用新的方法使用老的 创建传统的udev规则根据现有的初始化规则来创建bash /lib/udev/init-net-rules.shcat /etc/udev/rules.d/70-persistent-net.rules 具有相同功能的设备出现在 /dev 目录下的顺序是随机的。假如你有一个 USB 摄像头和一个电视调谐器，/dev/video0 有可能是 USB 摄像头，/dev/video1 是电视调谐器，有时候又可能是反过来的。对于除声卡和网卡外的设备，都可以通过创建自定义持久性符号链接的 udev 规则来固定。 处理类似的设备对于你所有的硬件，都有可能遇到此问题（尽管此问题可能在你当前的 Linux 发行版上不存在），在 /sys/class 或 /sys/block 目录下找到对应目录，比如，显卡可能的路径为 /sys/class/video4linux/videoX。找到该设备的唯一设备标识（通常，厂商和产品 ID 以及/或 序列号会有用）：udevadm info -a -p /sys/class/video4linux/video0然后通过写入规则建立符号链接：12345678cat &gt; /etc/udev/rules.d/83-duplicate_devs.rules &lt;&lt; &quot;EOF&quot;# Persistent symlinks for webcam and tunerKERNEL==&quot;video*&quot;, ATTRS&#123;idProduct&#125;==&quot;1910&quot;, ATTRS&#123;idVendor&#125;==&quot;0d81&quot;, \ SYMLINK+=&quot;webcam&quot;KERNEL==&quot;video*&quot;, ATTRS&#123;device&#125;==&quot;0x036f&quot;, ATTRS&#123;vendor&#125;==&quot;0x109e&quot;, \ SYMLINK+=&quot;tvtuner&quot;EOF 最终，/dev/video0 和 /dev/video1 依旧会随机分配给 USB 摄像头和电视调谐器，但是 /dev/tvtuner 和 /dev/webcam 将会固定的分配给正确的设备。 通用网络设置网络脚本启动和关闭哪些接口通常取决于/etc/sysconfig/中的文件。此目录应包含要配置的每个接口的文件，例如ifconfig.xyz，其中“xyz”应描述网卡。使用ip link 或者 ls /sys/class/net 查看接口名称 通过配置文件配置接口(静态ip)具体按照自己情况来12345678910cd /etc/sysconfig/cat &gt; ifconfig.eth0 &lt;&lt; &quot;EOF&quot;ONBOOT=yes 是否在booting的时候调用nic(网卡全称 network interface card)IFACE=eth0 接口名称SERVICE=ipv4-static 获取IP地址的方法IP=192.168.1.2GATEWAY=192.168.1.1 默认网关IP地址PREFIX=24 网络掩码BROADCAST=192.168.1.255 广播地址EOF For more information see the ifup man page. DNS解析通过将ISP服务器或网络管理员提供的DNS服务器的IP地址放入/etc/resolv.conf来实现ip地址的解析。123456789cat &gt; /etc/resolv.conf &lt;&lt; &quot;EOF&quot;# Begin /etc/resolv.confdomain &lt;Your Domain Name&gt;nameserver &lt;IP address of your primary nameserver&gt;nameserver &lt;IP address of your secondary nameserver&gt;# End /etc/resolv.confEOF Google 公开 IPv4 DNS 解析服务器地址为 8.8.8.8 和 8.8.4.4。译者注：国内也有一些 IT 公司提供公开可用的 DNS 解析服务：114 DNS：114.114.114.114 和 114.114.115.115阿里 DNS：223.5.5.5 和 223.6.6.6百度 DNS：180.76.76.76OpenDNS：208.67.220.220） 自定义host文件(一般是自动从dns的服务器中得到缓存,也可以手动配置)就算没有网卡，也应该提供有效的完整域名，否则某些软件可能无法正常运行。1234Private Network Address Range Normal Prefix10.0.0.1 - 10.255.255.254 8172.x.0.1 - 172.x.255.254 16192.168.y.1 - 192.168.y.254 24 x can be any number in the range 16-31. y can be any number in the range 0-255. System V BootscriptSysVinit(init)默认处于3的级别12345670: halt the computer 关闭计算机1: single-user mode 单人模式2: multi-user mode without networking 多人单机3: multi-user mode with networking 多人联机4: reserved for customization, otherwise does the same as 35: same as 4, it is usually used for GUI login (like X&apos;s xdm or KDE&apos;s kdm)6: reboot the computer 重启 内核初始化的时候，无论是命令行指定运行的第一个程序，还是默认的 init。该程序会读入初始化文件 /etc/inittab初始化文件的解释可以参考 inittab 的 man 手册页面。对于LFS，运行的核心命令是 rc。上面的初始化文件将指示 rc 运行 /etc/rc.d/rcS.d 目录中，所有以 S 开头的脚本，然后便是 /etc/rc.d/rc?.d 目录中，所有以 S 开头的脚本。目录中的问号由指定的 initdefault 值来决定。 为了方便，rc 会从 /lib/lsb/init-functions 中读取函数库。该库还会读取一个可选的配置文件 /etc/sysconfig/rc.site。任何在后续章节中描述到的系统配置文件中的参数，都可以放在这个文件中，允许将所有的系统参数合并到该文件中。 为了调试方便，函数脚本会将日志输出到 /run/var/bootlog 文件中。由于 /run 目录是个 tmpfs（临时文件系统），所以该文件在启动之后就不会持续存在了，但在启动过程的最后，这些内容会被添附到更为持久的 /var/log/boot.log 文件中。 想要改变运行级，可以使用命令 init &lt;runlevel&gt;，其中的 &lt;runlevel&gt; 便是想要切换到的运行级。举个例子，若是想要重启计算机，可以使用命令 init 6，这是 reboot 命令的别名。就像，init 0 是 halt 的别名一样。 对于中文，日语，韩语以及一些其他的语言需要的字符，Linux 的控制台还无法通过配置是之正常显示。用户若要使用这些语言，需要安装 X Window 系统，用于扩充所需字符域的字体，以及合适的输入法. 可选的 /etc/sysconfig/rc.site 文件中包含着那些为每个 System V 启动脚本自动设置好的设定。这些设定也可以在 /etc/sysconfig/ 目录中的 hostname，console，和clock 文件中设置。如果这些设定值同时在以上的这些文件和 rc.site 中设定了，那么脚本中的设定值将会被优先采用 自定义启动和关闭的脚本 LFS 启动脚本会以相当效率的方式启动和关闭系统，不过你可以在 rc.site 文件中进行调整，来提升速度或是根据需求调整消息。若是有这样的需求，就去调整上面 /etc/sysconfig/rc.site 文件的设置吧！在启动脚本 udev 运行时，会调用一次 udev settle，完成检测需要很长时间。这段时间根据当前系统的设备，可花可不花。如果你需要的仅仅是简单的分区和单个网卡，在启动的过程中，就没有必要等待这个命令执行。通过设置变量 OMIT_UDEV_SETTLE=y，可以跳过此命令。启动脚本udev_retry 默认也执行udev settle。该命令仅在 /var 目录是分开挂载的情况下需要。因为这种情况下时钟需要文件 /var/lib/hwclock/adjtime。其他的自定义设置可能也需要等待 udev 执行完成，但是在许多的安装中不需要。设置变量 OMIT_UDEV_RETRY_SETTLE=y 跳过命令。默认情况下，文件系统检测静默执行。看上去就像是启动过程中的一个延迟。想要打开 fsck 的输出，设置变量。重起时，你可能想完全的跳过文件系统检测 fsck。为此，可以创建 /fastboot 文件或是以 /sbin/shutdown -f -r now 命令重启系统。另一方面，你也可以通过创建 /forcefsck，或是在运行 shutdown 命令时，用 -F 参数代替-f，以此来强制检测整个文件系统。设置变量 FASTBOOT=y 将禁用启动过程中的 fsck，直至你将其移除。不推荐长时间地使用该方式。通常，/tmp 目录中的所有文件会在启动时删除。根据存在目录与文件的数量，该操作会导致启动过程中明显的延迟。如果要跳过移除文件的操作，设置变量 SKIPTMPCLEAN=y。在关机的过程中，init 程序会向每一个已经启动的程序（例如，agetty）发送一个 TERM 信号，等一段时间（默认为 3 秒），然后给每个进程发送 KILL 信号。对没有被自身脚本关闭的进程，sendsignals 脚本会重复此过程。init 的延迟可以通过参数来设置。比方说，想去掉 init 的延迟，可以通过在关机或重启时使用 -t0 参数（如，/sbin/shutdown -t0 -r now）。sendsignals 脚本的延迟可以通过设置参数 KILLDELAY=0 跳过。 创建 /etc/inputrc 文件 可编辑控制行的语句创建 /etc/shells 文件 shell的类似索引的东西]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[观药神有感]]></title>
    <url>%2F2018%2F07%2F25%2Fxiaoshi2%2F</url>
    <content type="text"><![CDATA[从迷雾中来黑暗指引着我但即使我步入光明也仍是黑暗的子嗣我体会过胆怯也害怕孤独但为了亲人我竭尽全力前行逆行中我见过许多红的白的原以为这一切与我无关但我还是能感受到懊悔愤怒从我那丁点的左胸膛迸发我们摆脱不来穷病也脱离不了社会而我能做的只是用我的脊梁去分担那微不足道的分量]]></content>
      <categories>
        <category>诗歌集</category>
      </categories>
      <tags>
        <tag>小诗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-07-18 lfs编译日志]]></title>
    <url>%2F2018%2F07%2F18%2F2018-07-18%2F</url>
    <content type="text"><![CDATA[主要在写md文档,有点无聊,写了几天只写了20个左右的工具编译打算不写了,主要还是看文档,之后是配置系统的过程]]></content>
      <categories>
        <category>lfs编译日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux For Scratch Pass 4]]></title>
    <url>%2F2018%2F07%2F18%2Flfs4%2F</url>
    <content type="text"><![CDATA[安装基本系统软件过程(重点过程)部分在中文版中没有,取自英文版 Linux-4.15.3 API HeadersLinux API 头文件（在 linux-3.19.tar.xz 里）会将内核 API 导出给 Glibc 使用。 Linux 内核需要提供一个应用编程接口（API）供系统的 C 库（LFS 中的 Glibc）调用。这通过整理 Linux 内核源码包中的多个 C 头文件来完成。 确保在之前的动作里没有留下旧文件和依赖关系：make mrproper现在要从源代码里解压出用户需要的内核头文件。因为解压过程会删除目标目录下所有文件，所以我们会先输出到一个本地中间目录后再拷贝到需要的地方。而且里面还有一些隐藏文件是给内核开发人员用的，而 LFS 不需要，所以会将它们从中间目录里删除。 123make INSTALL_HDR_PATH=dest headers_installfind dest/include \( -name .install -o -name ..install.cmd \) -deletecp -rv dest/include/* /usr/include Man-pages-4.15Man-pages 软件包里包含了超过 2,200 份 man 手册页面。 Install Man-pages by running:make install Glibc-2.21Glibc 软件包包含了主要的 C 函数库。这个库提供了分配内存、搜索目录、打开关闭文件、读写文件、操作字符串、模式匹配、基础算法等基本程序。注意有些 LFS 之外的软件包会建议安装 GNU libiconv 来转换不同编码的文字。根据项目主页（http://www.gnu.org/software/libiconv/ ）上的说法“这个库会提供函数 iconv() 的实现，应用于那些没有这个函数的系统，或者函数实现中不支持 Unicode 转换的系统。” Glibc 提供了函数 iconv() 的实现而且支持 Unicode 转换，所以对于 LFS 系统来说并不需要 libiconv 库。 有些 Glibc 程序会用到和 FHS 不兼容的 /var/db 目录来存储它们的运行时数据。打上如下的补丁让这些程序在 FHS 兼容的位置存储它们的运行时数据。 patch -Np1 -i ../glibc-2.21-fhs-1.patch 创建链接:ln -sfv /tools/lib/gcc /usr/lib 创建系统链接(create a symlink for LSB compliance):123456789case $(uname -m) in i?86) GCC_INCDIR=/usr/lib/gcc/$(uname -m)-pc-linux-gnu/7.3.0/include ln -sfv ld-linux.so.2 /lib/ld-lsb.so.3 ;; x86_64) GCC_INCDIR=/usr/lib/gcc/x86_64-pc-linux-gnu/7.3.0/include ln -sfv ../lib/ld-linux-x86-64.so.2 /lib64 ln -sfv ../lib/ld-linux-x86-64.so.2 /lib64/ld-lsb-x86-64.so.3 ;;esac 删除前面可能遗留的文件:rm -f /usr/include/limits.h 编译:1234567CC=&quot;gcc -isystem $GCC_INCDIR -isystem /usr/include&quot; \../configure --prefix=/usr \ --disable-werror \ --enable-kernel=3.2 \ --enable-stack-protector=strong \ libc_cv_slibdir=/libunset GCC_INCDIR CC=&quot;gcc -isystem $GCC_INCDIR -isystem /usr/include&quot;Setting the location of both gcc and system include directories avoids introduction of invalid(无效) paths in debugging symbols. --disable-werrorThis option disables the -Werror option passed to GCC. This is necessary for running the test suite. --enable-stack-protector=strongThis option increases system security by adding extra code to check for buffer overflows(缓冲溢出), such as stack smashing attacks. libc_cv_slibdir=/libThis variable sets the correct library for all systems. We do not want lib64 to be used. 在安装 Glibc 时会抱怨找不到/etc/ld.so.conf文件，这只是无关紧要的输出信息。下面的方式可以避免这个警告：touch /etc/ld.so.conf Fix the generated Makefile to skip an unneeded sanity check that fails in the LFS partial environment: sed &#39;/test-installation/s@$(PERL)@echo not running@&#39; -i ../Makefile Install the configuration file and runtime directory for nscd: cp -v ../nscd/nscd.conf /etc/nscd.confmkdir -pv /var/cache/nscd 安装语言环境上面的命令并没有安装可以让你的电脑用不同语言响应的语言环境。语言环境并不是必须的，只是如果有些语言环境缺失，后续的测试套件可能会跳过一些重要测试用例。 单独的语言环境可以用 localedef 程序安装。例如，下面第一个 localedef 命令将 /usr/share/i18n/locales/cs_CZ 字符无关的语言环境定义和 /usr/share/i18n/charmaps/UTF-8.gz 字符表定义组合在一起，并将结果附加到 /usr/lib/locale/locale-archive 文件末尾。下面的命令将安装能完美覆盖测试所需语言环境的最小集合：12345678910111213141516171819202122mkdir -pv /usr/lib/localelocaledef -i cs_CZ -f UTF-8 cs_CZ.UTF-8localedef -i de_DE -f ISO-8859-1 de_DElocaledef -i de_DE@euro -f ISO-8859-15 de_DE@eurolocaledef -i de_DE -f UTF-8 de_DE.UTF-8localedef -i en_GB -f UTF-8 en_GB.UTF-8localedef -i en_HK -f ISO-8859-1 en_HKlocaledef -i en_PH -f ISO-8859-1 en_PHlocaledef -i en_US -f ISO-8859-1 en_USlocaledef -i en_US -f UTF-8 en_US.UTF-8localedef -i es_MX -f ISO-8859-1 es_MXlocaledef -i fa_IR -f UTF-8 fa_IRlocaledef -i fr_FR -f ISO-8859-1 fr_FRlocaledef -i fr_FR@euro -f ISO-8859-15 fr_FR@eurolocaledef -i fr_FR -f UTF-8 fr_FR.UTF-8localedef -i it_IT -f ISO-8859-1 it_ITlocaledef -i it_IT -f UTF-8 it_IT.UTF-8localedef -i ja_JP -f EUC-JP ja_JPlocaledef -i ru_RU -f KOI8-R ru_RU.KOI8-Rlocaledef -i ru_RU -f UTF-8 ru_RU.UTF-8localedef -i tr_TR -f UTF-8 tr_TR.UTF-8localedef -i zh_CN -f GB18030 zh_CN.GB18030 另外，安装适合你自己国家、语言和字符集的语言环境。 或者，也可以一次性安装在 glibc-2.21/localedata/SUPPORTED 文件里列出的所有语言环境（包括以上列出的所有语言环境以及其它更多），执行下面这个非常耗时的命令：make localedata/install-locales你需要的语言环境几乎不大可能没列在 glibc-2.21/localedata/SUPPORTED 文件中，但如果真的没有可以使用 localedef 命令创建和安装。 配置 Glibc尽管 Glibc 在文件 /etc/nsswitch.conf 丢失或损坏的情况下会创建一个默认的，但是我们需要手动该创建文件，因为 Glibc 的默认文件在网络环境下工作时有问题。另外，也需要设置一下时区。 运行下面的命令创建一个新文件 /etc/nsswitch.conf： 1234567891011121314151617cat &gt; /etc/nsswitch.conf &lt;&lt; &quot;EOF&quot;#Begin /etc/nsswitch.confpasswd: filesgroup: filesshadow: fileshosts: files dnsnetworks: filesprotocols: filesservices: filesethers: filesrpc: files#End /etc/nsswitch.confEOF 安装时区数据： 123456789101112131415tar -xf ../../tzdata2018c.tar.gzZONEINFO=/usr/share/zoneinfomkdir -pv $ZONEINFO/&#123;posix,right&#125;for tz in etcetera southamerica northamerica europe africa antarctica \ asia australasia backward pacificnew systemv; do zic -L /dev/null -d $ZONEINFO -y &quot;sh yearistype.sh&quot; $&#123;tz&#125; zic -L /dev/null -d $ZONEINFO/posix -y &quot;sh yearistype.sh&quot; $&#123;tz&#125; zic -L leapseconds -d $ZONEINFO/right -y &quot;sh yearistype.sh&quot; $&#123;tz&#125;donecp -v zone.tab zone1970.tab iso3166.tab $ZONEINFOzic -d $ZONEINFO -p America/New_Yorkunset ZONEINFO zic -L /dev/null ...这会创建没有时间补偿的 posix 时区数据。一般将它们同时放在 zoneinfo 和 zoneinfo/posix 目录下。另外需要将 POSIX 时区数据放到 zoneinfo 目录下，否则很多测试套件会报错。在嵌入式平台，如果存储空间紧张而且你也不准备更新时区，也可以不用 posix 目录从而节省 1.9MB，但是一些应用程序或测试套件也许会出错。 zic -L leapseconds ...这会创建包含时间补偿的 right 时区数据。在嵌入式平台，空间比较紧张而且你也不打算更新时区或者不需要准确时间，你可以忽略 right 目录从而节省 1.9MB。 zic ... -p ...这会创建 posixrules 文件。我们使用纽约是因为 POSIX 要求夏令时规则与 US 标准一致。 一种确定本地时区的方式是运行下面的脚本：tzselect 然后运行下面的命令创建 /etc/localtime 文件：ln -sfv /usr/share/zoneinfo/&lt;xxx&gt; /etc/localtime将命令中的 替换成你所在实际时区的名字（比如 Canada/Eastern）。我的是 Asia/Shanghai. 配置动态库加载器默认情况下，动态库加载器（/lib/ld-linux.so.2）会搜索目录 /lib 和 /usr/lib 查找程序运行时所需的动态库文件。不过，如果库文件不在 /lib 和 /usr/lib 目录下，需要把它所在目录加到 /etc/ld.so.conf 文件里，保证动态库加载器能找到这些库。通常有两个目录包含额外的动态库，/usr/local/lib 和 /opt/lib，把这两个目录加到动态库加载器的搜索路径中。 运行下面的命令创建一个新文件/etc/ld.so.conf：123456cat &gt; /etc/ld.so.conf &lt;&lt; &quot;EOF&quot;# Begin /etc/ld.so.conf/usr/local/lib/opt/libEOF 如果需要的话，动态库加载器也可以查找目录并包含里面配置文件的内容。通常在这个包含目录下的文件只有一行字指向库目录。运行下面的命令增加这个功能： 1234567cat &gt;&gt; /etc/ld.so.conf &lt;&lt; &quot;EOF&quot;# Add an include directoryinclude /etc/ld.so.conf.d/*.confEOFmkdir -pv /etc/ld.so.conf.d 调整工具链(测试)现在最后的 C 语言库已经装好了，是时候调整工具链，让新编译的程序链接到这些新的库上。 首先,备份 /tools 链接器，然后用我们在第五章调整过的链接器代替它。我们还会创建一个链接，链接到 /tools/$(gcc -dumpmachine)/bin 的副本： mv -v /tools/bin/{ld,ld-old}mv -v /tools/$(gcc -dumpmachine)/bin/{ld,ld-old}mv -v /tools/bin/{ld-new,ld}ln -sv /tools/bin/ld /tools/$(gcc -dumpmachine)/bin/ld 接下来，修改 GCC 参数文件，让它指向新的动态连接器。只需删除所有 “/tools” 的实例，这样应该可以留下到达动态链接器的正确路径。还要调整参数文件，这样 GCC 就知道怎样找到正确的头文件和 Glibc 启动文件。一个 sed 命令就能完成这些:1234gcc -dumpspecs | sed -e &apos;s@/tools@@g&apos; \-e &apos;/\*startfile_prefix_spec:/&#123;n;s@.*@/usr/lib/ @&#125;&apos; \-e &apos;/\*cpp:/&#123;n;s@$@ -isystem /usr/include@&#125;&apos; &gt; \`dirname $(gcc --print-libgcc-file-name)`/specs 确保已调整的工具链的基本功能（编译和链接）都能如期进行是非常必要的。 怎样做呢？执行下面这条命令：123echo &apos;main()&#123;&#125;&apos; &gt; dummy.ccc dummy.c -v -Wl,--verbose &amp;&gt; dummy.logreadelf -l a.out | grep &apos;: /lib&apos; 如果没有任何错误，上条命令的输出应该是（不同的平台上的动态链接器可能名字不同）： [Requesting program interpreter: /lib/ld-linux.so.2]注意 /lib 现在是我们动态链接库的前缀。 现在确保我们已经设置好了启动文件：grep -o &#39;/usr/lib.*/crt[1in].*succeeded&#39; dummy.log上一条命令的输出应该是：123/usr/lib/crt1.o succeeded/usr/lib/crti.o succeeded/usr/lib/crtn.o succeeded 确保链接器能找到正确的头文件：grep -B1 &#39;^ /usr/include&#39; dummy.log这条命令应该返回如下输出：12#include &lt;...&gt; search starts here: /usr/include 接下来，确认新的链接器已经在使用正确的搜索路径： grep &#39;SEARCH.*/usr/lib&#39; dummy.log |sed &#39;s|; |\n|g&#39;应该忽略指向带有 ‘-linux-gnu’ 的路径，上条命令的输出应该是：12SEARCH_DIR(&quot;/usr/lib&quot;)SEARCH_DIR(&quot;/lib&quot;); 然后我们要确定我们使用的是正确的 libc:grep &quot;/lib.*/libc.so.6 &quot; dummy.log 上条命令的输出应该是（在 64 位主机上会有 lib64 目录）：attempt to open /lib/libc.so.6 succeeded 最后，确保 GCC 使用的是正确的动态链接器：grep found dummy.log 上条命令的结果应该是（不同的平台上链接器名字可以不同，64 位主机上是 lib64 目录）：found ld-linux.so.2 at /lib/ld-linux.so.2 如果显示的结果不一样或者根本没有显示，那就出了大问题。检查并回溯之前的步骤，找到出错的地方并改正。最有可能的原因是参数文件的调整出了问题。在进行下一步之前所有的问题都要解决。一旦所有的事情都正常了，清除测试文件：rm -v dummy.c a.out dummy.log Zlib-1.2.11Zlib 软件包包括一些程序所使用的压缩和解压缩例程。 共享库需要移动到 /lib，因此需要重建 /usr/lib 里面的 .so 文件：12mv -v /usr/lib/libz.so.* /libln -sfv ../../lib/$(readlink /usr/lib/libz.so) /usr/lib/libz.so File-5.22File 软件包包括一个判断给定的某个或某些文件文件类型的工具。 Readline-7.0Readline 软件包是一个提供命令行编辑和历史能力的一些库 sed -i &#39;/MV.*old/d&#39; Makefile.insed -i &#39;/{OLDSUFF}/c:&#39;support/shlib-install 编译:123./configure --prefix=/usr \ --disable-static \ --docdir=/usr/share/doc/readline-7.0 1make SHLIB_LIBS=&quot;-L/tools/lib -lncursesw&quot; SHLIB_LIBS=&quot;-L/tools/lib -lncursesw&quot;This option forces Readline to link against the libncursesw library. 转移动态库到合适位置,修复一些链接符号:123mv -v /usr/lib/lib&#123;readline,history&#125;.so.* /libln -sfv ../../lib/$(readlink /usr/lib/libreadline.so) /usr/lib/libreadline.soln -sfv ../../lib/$(readlink /usr/lib/libhistory.so ) /usr/lib/libhistory.so 如果需要的话,安装文档:install -v -m644 doc/*.{ps,pdf,html,dvi} /usr/share/doc/readline-7.0 M4-1.4.18M4软件包包含一个宏处理器 Bc-1.07.1Bc软件包用于数学处理change an internal script to use sed instead of ed:1234567891011cat &gt; bc/fix-libmath_h &lt;&lt; &quot;EOF&quot;#! /bin/bashsed -e &apos;1 s/^/&#123;&quot;/&apos; \ -e &apos;s/$/&quot;,/&apos; \ -e &apos;2,$ s/^/&quot;/&apos; \ -e &apos;$ d&apos; \ -i libmath.hsed -e &apos;$ s/$/0&#125;/&apos; \ -i libmath.hEOF Create temporary symbolic links so the package can find the readline library and confirm that its required libncurses library is available. Even though the libraries are in /tools/lib at this point, the system will use /usr/lib at the end of this chapter.12ln -sv /tools/lib/libncursesw.so.6 /usr/lib/libncursesw.so.6ln -sfv libncurses.so.6 /usr/lib/libncurses.so Fix an issue in configure due to missing files in the early stages of LFS:sed -i -e &#39;/flex/s/as_fn_error/: ;; # &amp;/&#39; configure编译:1234./configure --prefix=/usr \ --with-readline \ --mandir=/usr/share/man \ --infodir=/usr/share/info --with-readlineThis option tells Bc to use the readline library that is already installed on the system rather than using its own readline version. To test bc, run the commands below. There is quite a bit of output, so you may want to redirect it to a file. There are a very small percentage of tests (10 of 12,144) that will indicate a round off error at the last digit.echo &quot;quit&quot; | ./bc/bc -l Test/checklib.b Binutils-2.25Binutils 软件包包含一个链接器、一个汇编器、以及其它处理目标文件的工具。验证:expect -c &quot;spawn ls“正常输出:spawn ls假如输出包括下面的信息，那么表示没有为 PTY 操作设置好环境。在运行 Binutils 和 GCC 的测试套件之前需要解决这个问题：12The system has no more ptys.Ask your system administrator to create more. 编译:12345678../configure --prefix=/usr \ --enable-gold \ --enable-ld=default \ --enable-plugins \ --enable-shared \ --disable-werror \ --enable-64-bit-bfd \ --with-system-zlib --enable-goldBuild the gold linker and install it as ld.gold (along side the default linker). --enable-ld=defaultBuild the original bdf linker and install it as both ld (the default linker) and ld.bfd. --enable-pluginsEnables plugin support for the linker. --enable-64-bit-bfdEnables 64-bit support (on hosts with narrower word sizes). May not be needed on 64-bit systems, but does no harm. --with-system-zlibUse the installed zlib library rather than building the included version. 编译:make tooldir=/usr tooldir=/usr一般来说，tooldir (最终存放可执行文件的目录) 设置为 $(exec_prefix)/$(target_alias)。例如,x86_64机器会把它扩展为/usr/x86_64-unknown-linux-gnu。因为这是个自定制的系统，并不需要 /usr 中的特定目标目录。如果系统用于交叉编译（例如，在 Intel 机器上编译能生成在 PowerPC 机器上运行的代码的软件包）会使用 $(exec_prefix)/$(target_alias)。 测试make -k check安装:make tooldir=/usr install GMP-6.1.2GMP 软件包包含一些数学库。这里有对任意精度数值计算很有用的函数。 如果你是为 32 位的 x86 系统编译，但是你的 CPU 可以运行 64 位代码 而且 环境中你有指定的 CFLAGS，那么配置脚本会尝试配置为 64 位并导致失败。用下面的方式执行配置命令来避免这个问题:ABI=32 ./configure ... The default settings of GMP produce libraries optimized for the host processor. If libraries suitable for processors less capable than the host’s CPU are desired,(库对于处理器的适合度小于宿主的cpu的能力,然后创建通用的库),generic libraries can be created by running the following:12cp -v configfsf.guess config.guesscp -v configfsf.sub config.sub --enable-cxx这个参数启用 C++ 支持 --docdir=/usr/share/doc/gmp-6.1.2这个变量指定保存文档的正确位置。 Ensure that all 190 tests in the test suite passed.awk &#39;/# PASS:/{total+=$3} ; END{print total}&#39; gmp-check-log 1234./configure --prefix=/usr \ --enable-cxx \ --disable-static \ --docdir=/usr/share/doc/gmp-6.1.2 MPFR-4.0.1编译:1234./configure --prefix=/usr \ --disable-static \ --enable-thread-safe \ --docdir=/usr/share/doc/mpfr-4.0.1 MPC-1.1.0编译:123./configure --prefix=/usr \ --disable-static \ --docdir=/usr/share/doc/mpc-1.1.0 GCC-7.3.0If building on x86_64, change the default directory name for 64-bit libraries to “lib”:123456case $(uname -m) in x86_64) sed -e &apos;/m64=/s/lib64/lib/&apos; \ -i.orig gcc/config/i386/t-linux64 ;;esac Remove the symlink created earlier as the final gcc includes will be installed here:rm -f /usr/lib/gcc 编译:123456SED=sed \../configure --prefix=/usr \ --enable-languages=c,c++ \ --disable-multilib \ --disable-bootstrap \ --with-system-zlib SED=sedSetting this environment variable prevents a hard-coded path to /tools/bin/sed. --with-system-zlibThis switch tells GCC to link to the system installed copy of the Zlib library, rather than its own internal copy. One set of tests in the GCC test suite is known to exhaust the stack, so increase the stack size prior to running the tests:(一个测试会用尽tests,需要扩大栈的容量)ulimit -s 32768一些意料之外的错误总是难以避免。GCC 开发者通常意识到了这些问题，但还没有解决。除非测试结果和上面 URL 中的相差很大，不然就可以安全继续。 On some combinations of kernel configuration and AMD processors there may be more than 1100 failures in the gcc.target/i386/mpx tests (which are designed to test the MPX option on recent Intel processors). These can safely be ignored on AMD processors.我表示震惊…. 一些软件包希望 GCC 安装在 /lib 目录。为了支持那些软件包，可以建立一个符号链接： ln -sv ../usr/bin/cpp /lib译者注：如果还在 gcc-build 目录，这里应该是 ln -sv ../../usr/bin/cpp /lib 。很多软件包用命令 cc 调用 C 编译器。为了满足这些软件包，创建一个符号链接：ln -sv gcc /usr/bin/cc 增加一个兼容符号链接启用编译程序时进行链接时间优化（Link Time Optimization，LTO）：123install -v -dm755 /usr/lib/bfd-pluginsln -sfv ../../libexec/gcc/$(gcc -dumpmachine)/7.3.0/liblto_plugin.so \ /usr/lib/bfd-plugins/ 然后进行检验(步骤省略)最后，移动位置放错的文件：12mkdir -pv /usr/share/gdb/auto-load/usr/libmv -v /usr/lib/*gdb.py /usr/share/gdb/auto-load/usr/lib Bzip2-1.0.6Bzip2 软件包包含压缩和解压缩的程序。用 bzip2 压缩文本文件能获得比传统的 gzip 更好的压缩比。使用能为这个软件包安装帮助文档的补丁：patch -Np1 -i ../bzip2-1.0.6-install_docs-1.patch 下面的命令确保安装的符号链接是相对链接：sed -i &#39;s@\(ln -s -f \)$(PREFIX)/bin/@\1@&#39; Makefile 确认 man 页面安装到了正确的位置：sed -i &quot;s@(PREFIX)/man@(PREFIX)/share/man@g&quot; Makefile 编译:12make -f Makefile-libbz2_somake clean -f Makefile-libbz2_so这会使用不同的 Makefile 文件编译 Bzip2，在这里是 Makefile-libbz2_so，它会创建动态 libbz2.so 库，并把它链接到 Bzip2 工具。 安装使用动态链接库的 bzip2 二进制文件到 /bin 目录， 创建一些必须的符号链接并清理：123456cp -v bzip2-shared /bin/bzip2cp -av libbz2.so* /libln -sv ../../lib/libbz2.so.1.0 /usr/lib/libbz2.sorm -v /usr/bin/&#123;bunzip2,bzcat,bzip2&#125;ln -sv bzip2 /bin/bunzip2ln -sv bzip2 /bin/bzcat Pkg-config-0.29.2pkg-config 软件包包含一个在配置和 make 文件运行时把 include 路径和库路径传递给编译工具的工具。1234./configure --prefix=/usr \ --with-internal-glib \ --disable-host-tool \ --docdir=/usr/share/doc/pkg-config-0.29.2 --with-internal-glib这会让 pkg-config 使用它自己内部版本的 Glib，因为在 LFS 中没有可用的外部版本。 --disable-host-tool这个选项取消创建到 pkg-config 程序的不必要的硬链接。 Ncurses-6.1Ncurses 软件包包含用于不依赖于特定终端的字符屏幕处理的库。Don’t install a static library that is not handled by configure:sed -i &#39;/LIBTOOL_INSTALL/d&#39; c++/Makefile.in 编译:1234567./configure --prefix=/usr \ --mandir=/usr/share/man \ --with-shared \ --without-debug \ --without-normal \ --enable-pc-files \ --enable-widec --enable-widec这个选项会编译宽字符库（例如 libncursesw.so.5.9）而不是常规的）例如 libncurses.so.5.9）。宽字符库可用于多字节和传统的 8 位本地字符， 而常规的库只能用于 8 位本地字符。宽字符库和常规的库是源文件兼容的，而不是二进制文件兼容的。 --enable-pc-files该选项为 pkg-config 生成和安装 .pc 文件。 --without-normal该选项取消生成与安装静态库 Move the shared libraries to the /lib directory, where they are expected to reside(转移库文件):mv -v /usr/lib/libncursesw.so.6* /lib Because the libraries have been moved, one symlink points to a non-existent file. Recreate it(重新链接库文件):ln -sfv ../../lib/$(readlink /usr/lib/libncursesw.so) /usr/lib/libncursesw.so 很多应用程序仍然希望编辑器能找到非宽字符的 Ncurses 库。通过符号链接和链接器脚本欺骗这样的应用链接到宽字符库：12345for lib in ncurses form panel menu ; do rm -vf /usr/lib/lib$&#123;lib&#125;.so echo &quot;INPUT(-l$&#123;lib&#125;w)&quot; &gt; /usr/lib/lib$&#123;lib&#125;.so ln -sfv $&#123;lib&#125;w.pc /usr/lib/pkgconfig/$&#123;lib&#125;.pcdone 最后，确保在编译时会查找 -lcurses 的旧应用程序仍然可以编译： 123rm -vf /usr/lib/libcursesw.soecho &quot;INPUT(-lncursesw)&quot; &gt; /usr/lib/libcursesw.soln -sfv libncurses.so /usr/lib/libcurses.so 注意上面的指令并不会创建非宽字符 Ncurses 库，因为没有从源文件中编译安装的软件包会在运行时链接它们。如果你由于一些仅有二进制的应用程序或要和 LSB 兼容而必须要有这样的库，用下面的命令重新编译软件包：123456789make distclean./configure --prefix=/usr \ --with-shared \ --without-normal \ --without-debug \ --without-cxx-binding \ --with-abi-version=5make sources libscp -av lib/lib*.so.5* /usr/lib Attr-2.4.47attr 软件包包含管理文件系统对象的扩展属性的工具。Modify the documentation directory so that it is a versioned directory:sed -i -e &#39;s|/@pkg_name@|&amp;-@pkg_version@|&#39; include/builddefs.in Prevent installation of manual pages that were already installed by the man pages package:sed -i -e &quot;/SUBDIRS/s|man[25]||g&quot; man/Makefile Fix a problem in the test procedures caused by changes in perl-5.26:(修复问题)sed -i &#39;s:{(:\\{(:&#39; test/run 123./configure --prefix=/usr \ --bindir=/bin \ --disable-static The shared library needs to be moved to /lib, and as a result the .so file in /usr/lib will need to be recreated:mv -v /usr/lib/libattr.so.* /libln -sfv ../../lib/$(readlink /usr/lib/libattr.so) /usr/lib/libattr.so Acl-2.2.52Modify the documentation directory so that it is a versioned directory: sed -i -e &#39;s|/@pkg_name@|&amp;-@pkg_version@|&#39; include/builddefs.inFix some broken tests: sed -i &quot;s:| sed.*::g&quot; test/{sbits-restore,cp,misc}.testFix a problem in the test procedures caused by changes in perl-5.26: sed -i &#39;s/{(/\\{(/&#39; test/runAdditionally, fix a bug that causes getfacl -e to segfault on overly long group name:12sed -i -e &quot;/TABS-1;/a if (x &gt; (TABS-1)) x = (TABS-1);&quot; \ libacl/__acl_to_any_text.c Prepare Acl for compilation:1234./configure --prefix=/usr \ --bindir=/bin \ --disable-static \ --libexecdir=/usr/lib Libcap-2.25Libcap 软件包实现了可用在 Linux 内核上的对 POSIX 1003.1e 功能的用户空间接口。 这些功能将所有强大 root 权限划分为不同的权限组合。Install the package:12make RAISE_SETFCAP=no lib=lib prefix=/usr installchmod -v 755 /usr/lib/libcap.so The meaning of the make option:RAISE_SETFCAP=noThis parameter skips trying to use setcap on itself. This avoids an installation error if the kernel or file system does not support extended capabilities. lib=libThis parameter installs the library in $prefix/lib rather than $prefix/lib64 on x86_64. It has no effect onx86. Sed-4.4The Sed package contains a stream editor.First fix an issue in the LFS environment and remove a failing test:sed -i &#39;s/usr/tools/&#39; build-aux/help2mansed -i &#39;s/testsuite.panic-tests.sh//&#39; Makefile.in Prepare Sed for compilation:./configure --prefix=/usr --bindir=/bin Shadow-4.5Shadow 软件包包含以安全方式处理密码的程序。注意如果你喜欢强制使用更强的密码，在编译 Shadow 之前可以根据 http://www.linuxfromscratch.org/blfs/view/systemd/postlfs/cracklib.html 安装 CrackLib。然后在下面的 configure 命令中增加 --with-libcrack。 取消安装 groups 程序以及它的 man 文档，因为 Coreutils 提供了一个更好的版本：1234sed -i &apos;s/groups$(EXEEXT) //&apos; src/Makefile.infind man -name Makefile.in -exec sed -i &apos;s/groups\.1 / /&apos; &#123;&#125; \;find man -name Makefile.in -exec sed -i &apos;s/getspnam\.3 / /&apos; &#123;&#125; \;find man -name Makefile.in -exec sed -i &apos;s/passwd\.5 / /&apos; &#123;&#125; \; 比起默认的 crypt 方法，用更安全的 SHA-512 方法加密密码，它允许密码长度超过 8 个字符。也需要把 Shadow 默认使用的用户邮箱由陈旧的 /var/spool/mail 位置改为正在使用的 /var/mail 位置12sed -i -e &apos;s@#ENCRYPT_METHOD DES@ENCRYPT_METHOD SHA512@&apos; \ -e &apos;s@/var/spool/mail@/var/mail@&apos; etc/login.defs Note如果你选择编译支持 Cracklib 的 Shadow，运行下面的命令：sed -i &#39;s@DICTPATH.*@DICTPATH\t/lib/cracklib/pw_dict@&#39; etc/login.defs 做个小的改动使 useradd 的默认设置和 LFS 的组文件一致：sed -i &#39;s/1000/999/&#39; etc/useradd Prepare Shadow for compilation:./configure --sysconfdir=/etc --with-group-name-max-length=32 The meaning of the configure option:--with-group-name-max-length=32最长用户名为 32 个字符，使组名称也是如此 Configuring Shadow该软件包包含增加、更改、以及删除用户和组的工具；设置和修改密码；执行其它特权级任务。软件包解压后的 doc/HOWTO 文件有关于 password shadowing 的完整解释。如果使用 Shadow 支持，记住需要验证密码（显示管理器、FTP 程序、pop3 守护进程等）的程序必须和 Shadow 兼容。 也就是说，它们要能使用 Shadow 加密的密码。 运行下面的命令启用 shadow 密码；pwconv 运行下面的命令启用 shadow 组密码：grpconv 用于 useradd 工具的 Shadow 配置有一些需要解释的注意事项。首先，useradd 工具的默认操作是创建用户以及和用户名相同的组。默认情况下，用户 ID(UID) 和组 ID(GID) 的数字从 1000 开始。这意味着如果你不传递参数给 useradd，系统中的每个用户都会属于一个不同的组。如果不需要这样的结果，你需要传递参数 -g 到 useradd。默认参数保存在 /etc/default/useradd 文件中。你需要修改该文件中的两个参数来实现你的特定需求。 /etc/default/useradd 参数解释 GROUP=1000该参数设定 /etc/group 文件中使用的起始组序号。你可以把它更改为任何你需要的数字。注意 useradd 永远不会重用 UID 或 GID。如果该参数指定的数字已经被使用了，将会使用它之后的下一个可用数字。另外注意如果你系统中没有序号为 1000 的组，第一次使用useradd 而没有参数 -g 的话，你会在终端中看到一个提示信息： useradd: unknown GID 1000。你可以忽视这个信息，它会使用组号 1000。 CREATE_MAIL_SPOOL=yes这个参数会为 useradd 新添加的用户创建邮箱文件。useradd 会使组 mail 拥有该文件的所有权，并赋予组 0660 的权限。如果你希望 useradd 不创建这些邮箱文件，你可以运行下面的命令：sed -i &#39;s/yes/no/&#39; /etc/default/useradd 设置 root 密码运行下面的命令为用户 root 设置密码：passwd root (由于构建文档太长,此篇到此结束,还有50多个工具的编译过程没有记录,主要还是要看构建文档)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[迷时]]></title>
    <url>%2F2018%2F07%2F17%2Fxiaoshi1%2F</url>
    <content type="text"><![CDATA[时光从不给人告白的机会慢慢的便失去了感觉直到掌心温度渐渐流逝眼眶才湿润起来到底我该以怎样的眼神看着你抉择中我总自以为抓住了时光却不想陷入了它的把戏可惜一生只够走一次只愿能给我留个念想]]></content>
      <categories>
        <category>诗歌集</category>
      </categories>
      <tags>
        <tag>小诗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-07-15 lfs编译日志]]></title>
    <url>%2F2018%2F07%2F15%2F2018-07-15%2F</url>
    <content type="text"><![CDATA[接上一次错误:发现是自己设备umount没有全部umount掉,然后就关机了结果出现了错误,经过一系列的操作后,正确这里简单的写一下:最开始是找到网上的fuser,结果发现没什么用最后使用mount,列出所有挂载点,将我u盘的挂载点逐一umount掉从最小的开始,最后成功 ps:gcc编译时长11小时,苦了我的电脑了cpu 70多度已经习以为常,编译工具链有点麻烦啊我现在进度还只有一半,后面的总结估计要好几天才能完成]]></content>
      <categories>
        <category>lfs编译日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keyboard_shortcuts]]></title>
    <url>%2F2018%2F07%2F14%2Fkeyboard-shortcuts%2F</url>
    <content type="text"><![CDATA[内核以下是系统底层的快捷键，通常被用于调试。遇到系统问题，请尽可能尝试这些快捷键，而不是按住电源开关强制关机。 这些快捷键需要首先使用如下命令激活echo &quot;1&quot; &gt; /proc/sys/kernel/sysrq如果你希望在系统启动时就开启，请编辑 /etc/sysctl.d/99-sysctl.conf 并添加配置 kernel.sysrq = 1. 如果你希望在挂载分区和启动引导前就开启的话, 请在内核启动参数上添加 sysrq_always_enabled=1. 记住这个激活命令的通用口诀是 “Reboot Even If System Utterly Broken” (或者”REISUB“)。 键盘快捷键 描述Alt+SysRq+R+ Unraw 从X收回对键盘的控制 Alt+SysRq+E+ Terminate 向所有进程发送SIGTERM信号，让它们正常终止 Alt+SysRq+I+ Kill 向所有进程发送SIGKILL信号，强制立即终止 Alt+SysRq+S+ Sync 将待写数据写入磁盘 Alt+SysRq+U+ Unmount 卸载所有硬盘然后重新按只读模式挂载 Alt+SysRq+B+ Reboot 重启]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux From Scratch Pass 3]]></title>
    <url>%2F2018%2F07%2F14%2Flfs3%2F</url>
    <content type="text"><![CDATA[安装基本的系统软件注意使用root用户 准备虚拟内核文件系统内核会挂载几个文件系统用于自己和用户空间程序交换信息。这些文件系统是虚拟的，并不占用实际磁盘空间，它们的内容会放在内存里。 开始先创建将用来挂载文件系统的目录： mkdir -pv $LFS/{dev,proc,sys,run} 创建初始设备节点在内核引导系统的时候，它依赖于几个设备节点，特别是 console 和 null 两个设备。这些设备节点需要创建在硬盘上，这样在 udevd 启动之前它们也仍然有效，特别是在 Linux 使用init=/bin/bash 参数启动的时候。运行下面的命令创建这几个设备节点： mknod -m 600 $LFS/dev/console c 5 1mknod -m 666 $LFS/dev/null c 1 3注: mknod创建设备文件 -m设置权限 挂载和激活 /dev通常激活 /dev 目录下设备的方式是在 /dev目录挂载一个虚拟文件系统（比如 tmpfs），然后允许在检测到设备或打开设备时在这个虚拟文件系统里动态创建设备节点。这个通常是在启动过程中由 Udev 完成。由于我们的新系统还没有 Udev 而且也没有被引导启动，有必要手动挂载和激活 /dev 目录。这可以通过绑定挂载宿主机系统的 /dev 目录实现。绑定挂载是一种特殊的挂载模式，它允许在另外的位置创建某个目录或挂载点的镜像。运行下面的命令来实现： mount -v --bind /dev $LFS/dev 挂载虚拟文件系统现在挂载剩下的虚拟内核文件系统：1234mount -vt devpts devpts $LFS/dev/pts -o gid=5,mode=620mount -vt proc proc $LFS/procmount -vt sysfs sysfs $LFS/sysmount -vt tmpfs tmpfs $LFS/run 注:mount [-fnrsvw] [-t vfstype] [-o options] device dir gid=5这个选项会让 devpts 创建的所有设备节点属主的组 ID 都是 5。这是我们待会将要指定给 tty 组的 ID。现在我们先用 ID代替组名，因为宿主机系统可能会为它的 tty 组分配了不同的 ID。 mode=0620这个选项会让 devpts 创建的所有设备节点的属性是 0620（属主用户可读写，组成员可写）。和上一个选项同时使用，可以保证 devpts 所创建的设备节点能满足 grantpt() 函数的要求，这意味着不需要 Glibc 的 pt_chown 帮助程序（默认没有安装）了。 在某些宿主机系统里，/dev/shm 是一个指向 /run/shm 的软链接。这个 /run 下的 tmpfs 文件系统已经在之前挂载了，所以在这里只需要创建一个目录。123if [ -h $LFS/dev/shm ]; then mkdir -pv $LFS/$(readlink $LFS/dev/shm)fi 升级问题软件包管理器可以在软件新版本发布后轻松升级。一般来说 LFS 和 BLFS 手册里的指令可以用来升级到新版本。下面是一些在你准备升级软件包时需要注意的事情，特别是在一个运行中的系统。 1.如果需要升级 Glibc 到新版本（比如，从 glibc-2.19 升级到 glibc-2.20），重新构建整个 LFS 会比较安全。虽然你也许能够按依赖关系重新编译所有的软件包，不过我们不建议这样做。 2.如果某个包含的动态库的软件包升级了，而且库名字有改变，那么所有动态链接到这个库的软件包都需要重新链接新的库。（请注意软件包版本和库名字并不存在相关性。）举个例子，某个软件包 foo-1.2.3 安装了一个名叫 libfoo.so.1 的动态库。然后假设你把这个软件包升级到了新版本 foo-1.2.4，而新版本会安装名叫 libfoo.so.2的动态库。在这种情况下，所有动态链接到 libfoo.so.1 的软件包都需要重新编译链接到 libfoo.so.2。注意在所有依赖软件包重新编译完成之前，请不要删除旧版的库文件。 创建软件包存档在这种方式里，像之前的软链接软件包管理方式里所描述的那样，软件包被伪装安装到一个独立的目录树里。在安装完成后，会将已安装文件打包成一个软件包存档。然后这个存档会用来在本地机器或其他机器上安装软件包。 这种方式为商业发行版中的大多数包管理器所采用。一些例子是 RPM（它顺便也是 Linux 标准规范 里所要求的）、pkg-utils、Debian 的 apt、以及 Gentoo 的 Portage 系统。该页面描述了如何在 LFS 系统里采用这种包管理方式： http://www.linuxfromscratch.org/hints/downloads/files/fakeroot.txt。 创建带有依赖关系的软件包存档非常复杂，已经超出 LFS 手册范围了。 基于用户的软件包管理在这种方式，是 LFS 特有的，由 Matthias Benkmann 所设计，可以在 Hints Project 里能找到。在这种方式里，每个软件包都由一个单独的用户安装到标准的位置。属于某个软件包的文件可以通过检查用户 ID 轻松识别出来。关于这种方式的特性和短处非常复杂，在本节里说不清楚。详细的信息请参看 http://www.linuxfromscratch.org/hints/downloads/files/more_control_and_pkg_man.txt。 在多个系统上布置 LFSLFS 系统的一个优点是没有会依赖磁盘系统里文件位置的文件。克隆一份 LFS 到和宿主机器相似配置的机器上，简单到只要对包含根目录的 LFS 分区（对于一个基本的 LFS 构建不压缩的话大概有 250MB）使用 tar命令打包，然后通过网络传输或光盘拷贝到新机器上展开即可。在这之后，还需要调整一些配置文件，包括：/etc/hosts、/etc/fstab、/etc/passwd、/etc/group、/etc/shadow 和 /etc/ld.so.conf。 根据系统硬件和原始内核配置文件的差异，可能还需要重新编译一下内核。 最后，需要使用 8.4 “用 GRUB 设置引导过程”里所介绍的方法让新系统可引导 进入 Chroot 环境123456chroot &quot;$LFS&quot; /tools/bin/env -i \ HOME=/root \ TERM=&quot;$TERM&quot; \ PS1=&apos;\u:\w\$ &apos; \ PATH=/bin:/usr/bin:/sbin:/usr/sbin:/tools/bin \ /tools/bin/bash --login +h(选项来关闭其哈希功能) 给 env 命令传递 -i 选项会清除这个 chroot 切换进去的环境里所有变量。随后，只重新设定了 HOME、TERM、PS1 和 PATH 变量。TERM=$TERM 语句会设定 chroot 进入的环境里的 TERM 变量为进入前该变量同样的值。许多程序需要这个变量才能正常工作，比如 vim 和 less。如果还需要设定其他变量，比如 CFLAGS 或 CXXFLAGS，就在这里一起设定比较合适。 从这里以后，就不再需要 LFS 变量了，因为后面所有工作都将被限定在 LFS 文件系统里。这是因为我们已经告诉 Bash 终端 $LFS 就是当前的根目录（/）。 请注意 /tools/bin 放在了 PATH 变量的最后。意思是在每个软件的最后版本编译安装好后就不再使用临时工具了。这还需要让 shell 不要“记住”每个可执行文件的位置—这样的话，还要给 bash 加上 +h 选项来关闭其哈希功能。 注意一下 bash 的提示符是 I have no name!。这是正常的，因为这个时候 /etc/passwd文件还没有被创建。 注意:非常重要，本章从这以后的命令，以及后续章节里的命令都要在 chroot 环境下运行。如果因为某种原因（比如说重启）离开了这个环境，请保证要按照 “挂载和激活 /dev” 和 “挂载虚拟内核文件系统” 里所说的那样挂载虚拟内核文件系统，然后在继续构建之前重新运行 chroot 进入环境。 创建目录123456789101112131415161718mkdir -pv /&#123;bin,boot,etc/&#123;opt,sysconfig&#125;,home,lib/firmware,mnt,opt&#125;mkdir -pv /&#123;media/&#123;floppy,cdrom&#125;,sbin,srv,var&#125;install -dv -m 0750 /rootinstall -dv -m 1777 /tmp /var/tmpmkdir -pv /usr/&#123;,local/&#125;&#123;bin,include,lib,sbin,src&#125;mkdir -pv /usr/&#123;,local/&#125;share/&#123;color,dict,doc,info,locale,man&#125;mkdir -v /usr/&#123;,local/&#125;share/&#123;misc,terminfo,zoneinfo&#125;mkdir -v /usr/libexecmkdir -pv /usr/&#123;,local/&#125;share/man/man&#123;1..8&#125;case $(uname -m) in x86_64) mkdir -v /lib64 ;;esacmkdir -v /var/&#123;log,mail,spool&#125;ln -sv /run /var/runln -sv /run/lock /var/lockmkdir -pv /var/&#123;opt,cache,lib/&#123;color,misc,locate&#125;,local&#125; 一般目录默认会按 755 的权限创建，但是这并不适用于所有的目录。在上面的命令里，有两个改动—一个是 root 用户的主目录，另一个是存放临时文件的目录。 第一个模式改动能保证不是所有人都能进入 /root目录—同样的一般用户也需要为他/她的主目录设置这样的模式。第二个模式改动能保证所有用户都可以写目录 /tmp 和 /var/tmp。还增加了一个所谓的 “粘滞位”的限制，即位掩码 0x1777 中最高位的比特(1)。 关于 FHS 兼容性这个目录树是基于文件系统目录结构标准（FHS）（参看 https://wiki.linuxfoundation.org/en/FHS) ,FHS 标准还规定了要有 /usr/local/games 和 /usr/share/games 目录。另外 FHS 标准关于/usr/local/share 里子目录的结构要求并不清晰，所以我们只创建了我们需要的目录。不过，如果你更喜欢严格遵守 FHS 标准，创建这些目录也不会有问题。 创建必需的文件和符号链接有些程序里会使用写死的路径调用其它暂时还未安装的程序。为了满足这种类型程序的需要，我们将创建一些符号链接，在完成本章内容后这些软件会安装好，并替代之前的符号链接:12345ln -sv /tools/bin/&#123;bash,cat,dd,echo,ln,pwd,rm,stty&#125; /binln -sv /tools/bin/&#123;install,perl&#125; /usr/binln -sv /tools/lib/libgcc_s.so&#123;,.1&#125; /usr/libln -sv /tools/lib/libstdc++.&#123;a,so&#123;,.6&#125;&#125; /usr/libln -sv bash /bin/sh 由于历史原因，Linux 在文件/etc/mtab中维护一个已挂载文件系统的列表。而现代内核改为在内部维护这个列表，并通过 /proc 文件系统输出给用户。为了满足一些依赖 /etc/mtab 文件的应用程序，我们要创建下面的符号链接：ln -sv /proc/self/mounts /etc/mtab为了让 root 用户能正常登录，而且 root 的名字能被正常识别，必须在文件 /etc/passwd 和 /etc/group 中写入相应的内容。 运行下面的命令创建 /etc/passwd 文件 1234567cat &gt; /etc/passwd &lt;&lt; &quot;EOF&quot;root:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/dev/null:/bin/falsedaemon:x:6:6:Daemon User:/dev/null:/bin/falsemessagebus:x:18:18:D-Bus Message Daemon User:/var/run/dbus:/bin/falsenobody:x:99:99:Unprivileged User:/dev/null:/bin/falseEOF root 用户的实际密码（这里的 “x” 只是占位符）将在后面创建。 运行下面的命令创建 /etc/group 文件：12345678910111213141516171819202122232425cat &gt; /etc/group &lt;&lt; &quot;EOF&quot;root:x:0:bin:x:1:daemonsys:x:2:kmem:x:3:tape:x:4:tty:x:5:daemon:x:6:floppy:x:7:disk:x:8:lp:x:9:dialout:x:10:audio:x:11:video:x:12:utmp:x:13:usb:x:14:cdrom:x:15:adm:x:16:messagebus:x:18:systemd-journal:x:23:input:x:24:mail:x:34:nogroup:x:99:users:x:999:EOF 这里创建的用户组没有参照任何标准 — 它们一部分是为了满足本章中配置 Udev 的需要，还有一部分来自一些现存 Linux 发行版的通用设定。另外，某些测试套件也依赖特定用户或组。而 Linux 标准规范 （LSB，参见http://www.linuxbase.org ）只要求以组 ID（GID）为 0 创建用户组 root 以及以 GID 为 1 创建用户组 bin。系统管理员可以自由分配其它所有用户组名字和 GID，因为优秀的程序不会依赖 GID 数字，而是使用组名。 为了移除 “I have no name!” 的提示符，可以打开一个新 shell。由于完整的 Glibc 已经在 第五章 里装好了，而且已经创建好了 /etc/passwd 和 /etc/group 文件，用户名和组名就可以正常解析了： exec /tools/bin/bash --login +h 注意这里使用了 +h 参数。这样会告诉 bash 不要使用它内建的路径哈希功能。而不加这个参数的话， bash 将会记住曾经执行过程序的路径。为了在新编译安装好程序后就能马上使用，参数 +h 将在本章中一直使用。 程序 login，agetty 和init（还有一些其它的）会使用一些日志文件来记录信息，比如谁在什么时候登录了系统。不过，在日志文件不存在的时候这些程序一般不会写入。下面初始化一下日志文件并加上合适的权限：1234touch /var/log/&#123;btmp,lastlog,wtmp&#125;chgrp -v utmp /var/log/lastlogchmod -v 664 /var/log/lastlogchmod -v 600 /var/log/btmp The /var/log/wtmp file records all logins and logouts.The /var/log/lastlog file records when each user last logged in.The /var/log/faillog file records failed login attempts.The /var/log/btmp file records the bad login attempts.文件 /run/utmp 会记录当前已登录的用户。这个文件会在启动脚本中动态创建。 接下来还是漫长的构建的过程,尤其是glibc和gcc (吐血)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-07-14 lfs编译日志]]></title>
    <url>%2F2018%2F07%2F14%2F2018-07-14%2F</url>
    <content type="text"><![CDATA[编译gcc出现错误:123456789In file included from ../../gcc-4.9.2/gcc/cp/except.c:1013:cfns.gperf:101:1: error: &apos;const char* libc_name_p(const char*, unsigned int)&apos; redeclared inline with &apos;gnu_inline&apos; attributecfns.gperf:26:14: note: &apos;const char* libc_name_p(const char*, unsigned int)&apos; previously declared herecfns.gperf:26:14: warning: inline function &apos;const char* libc_name_p(const char*, unsigned int)&apos; used but never definedmake[2]: *** [Makefile:1058: cp/except.o] Error 1make[2]: Leaving directory &apos;/mnt/lfs/sources/gcc-build/gcc&apos;make[1]: *** [Makefile:4027: install-gcc] Error 2make[1]: Leaving directory &apos;/mnt/lfs/sources/gcc-build&apos;make: *** [Makefile:2176: install] Error 2 原因 : 编译用的gcc版本太高 编译了一部分,电脑关机去睡觉,结果醒来接着编译的时候出错了…我估计着是chroot进去的时候出现了差异,然后出现错误真实让人头大,打算测试一下….有点心累同时比较幸运,还好不是在编完gcc的时候出现,不然时间浪费太严重了]]></content>
      <categories>
        <category>lfs编译日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tar基础]]></title>
    <url>%2F2018%2F07%2F14%2Ftar%2F</url>
    <content type="text"><![CDATA[记录: 来自网上 12345678910111213141516171、*.tar 用 tar -xvf 解压2、*.gz 用 gzip -d或者gunzip 解压3、*.tar.gz和*.tgz 用 tar -xzf 解压4、*.bz2 用 bzip2 -d或者用bunzip2 解压5、*.tar.bz2用tar -xjf 解压6、*.Z 用 uncompress 解压7、*.tar.Z 用tar -xZf 解压8、*.rar 用 unrar e解压9、*.zip 用 unzip 解压 我一般解压都是直接使用tar -xvf xxx这样比较省事吧 (=\ _ \ =) 然后压缩就是直接使用 tar -cvf xxx 了 加入有大小需求的话,我会考虑加上z,j之类的]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux From Scrach Pass 2]]></title>
    <url>%2F2018%2F07%2F14%2Flfs2%2F</url>
    <content type="text"><![CDATA[编译临时工具软件包过程说明:主要列出重要信息(来自官方),切忌不要按照本文编译我只是把它们列出来加深理解,还有一些配置我没有列出包括make与make install Binutils-2.25 - Pass 1Binutils 软件包包括了一个链接器、汇编器 和 其它处理目标文件 的工具。 1234567../binutils-2.25/configure \ --prefix=/tools \ --with-sysroot=$LFS \ --with-lib-path=/tools/lib \ --target=$LFS_TGT \ --disable-nls \ --disable-werror 配置选项的含义： --prefix=/tools 告诉配置脚本将 Binutils 程序安装到 /tools 文件夹。 --with-sysroot=$LFS 用于交叉编译，告诉编译系统在 $LFS 中查找所需的目标系统库。 --with-lib-path=/tools/lib 指定需要配置使用的链接器的库路径。 --target=$LFS_TGT (这个还不是很清楚) 因为 LFS_TGT 变量中的机器描述和 config.guess 脚本返回的值略有不同，这个选项会告诉 configure 脚本调整 Binutils 的编译系统来编译一个交叉链接器。 --disable-nls 这会禁止国际化（i18n），因为国际化对临时工具来说没有必要。 --disable-werror 这会防止来自宿主编译器的警告事件导致停止编译。 GCC-7.3.0 - Pass 1GCC 软件包是 GNU 编译器 集合的一部分，其中包括 C 和 C++ 的编译器。 下面的指令将会修改 GCC 默认的动态链接器为安装在 /tools 文件夹中的。它也会从 GCC 的 include 搜索路径中移除 /usr/include 12345678910111213for file in \ $(find gcc/config -name linux64.h -o -name linux.h -o -name sysv4.h)do cp -uv $file&#123;,.orig&#125; sed -e &apos;s@/lib\(64\)\?\(32\)\?/ld@/tools&amp;@g&apos; \ -e &apos;s@/usr@/tools@g&apos; $file.orig &gt; $file echo &apos;#undef STANDARD_STARTFILE_PREFIX_1#undef STANDARD_STARTFILE_PREFIX_2#define STANDARD_STARTFILE_PREFIX_1 &quot;/tools/lib/&quot;#define STANDARD_STARTFILE_PREFIX_2 &quot;&quot;&apos; &gt;&gt; $file touch $file.origdone 如果上面的看起来难以理解，让我们分开来看一下吧。首先我们找到 gcc/config 文件夹下的所有命名为 linux.h, linux64.h 或sysv4.h 的文件。对于找到的每个文件，我们把它复制到相同名称的文件，但增加了后缀 “.orig”。然后第一个 sed 表达式在每个 “/lib/ld”, “/lib64/ld” 或 “/lib32/ld” 实例前面增加“/tools”，第二个 sed 表达式替换 “/usr” 的硬编码实例。然后，我们添加这改变默认 startfile 前缀到文件末尾的定义语句。注意 “/tools/lib/” 后面的 “/” 是必须的。最后，我们用 touch 更新复制文件的时间戳。当与 cp -u 一起使用时，可以防止命令被无意中运行两次造成对原始文件意外的更改。(厉害厉害) 8.2版本中不用以下命令(可能修复了这个错误):12GCC 不能正确检测栈保护，这会导致编译 Glibc-2.21 时出现问题，用下面的命令修复这个问题：sed -i &apos;/k prot/agcc_cv_libc_provides_ssp=yes&apos; gcc/configure 准备编译 GCC:1234567891011121314151617181920212223../gcc-4.9.2/configure \ --target=$LFS_TGT \ --prefix=/tools \ --with-sysroot=$LFS \ --with-newlib \ --without-headers \ --with-local-prefix=/tools \ --with-native-system-header-dir=/tools/include \ --disable-nls \ --disable-shared \ --disable-multilib \ --disable-decimal-float \ --disable-threads \ --disable-libatomic \ --disable-libgomp \ --disable-libitm \ --disable-libquadmath \ --disable-libsanitizer \ --disable-libssp \ --disable-libvtv \ --disable-libcilkrts \ --disable-libstdc++-v3 \ --enable-languages=c,c++ --with-newlib由于还没有可用的 C 库，这确保编译 libgcc 时定义了常数 inhibit_libc。这可以防止编译任何需要 libc 支持的代码。 --without-headers创建一个完成的交叉编译器的时候，GCC 要求标准头文件和目标系统兼容。对于我们的目的来说，不需要这些头文件。这个选项可以防止 GCC 查找它们。 --with-local-prefix=/toolsGCC 会查找本地已安装的 include 文件的系统位置。默认是 /usr/local。把它设置为 /tools 能把主机位置中的 /usr/local 从 GCC 的搜索路径中排除。 --with-native-system-header-dir=/tools/includeGCC 默认会在/usr/include 中查找系统头文件。和 sysroot 选项一起使用，会转换为 $LFS/usr/include。在后面两个章节中头文件会被安装到 $LFS/tools/include。这个选项确保 gcc 能正确找到它们。第二次编译 GCC 时，同样的选项可以保证不会去寻找主机系统的头文件。 --disable-shared这个选项强制 GCC 静态链接到它的内部库。我们这样做是为了避免与主机系统可能出现的问题。 --disable-decimal-float, --disable-threads, --disable-libatomic, --disable-libgomp, --disable-libitm, --disable-libquadmath, --disable-libsanitizer, --disable-libssp, --disable-libvtv, --disable-libcilkrts, --disable-libstdc++-v3这些选项取消了对十进制浮点数扩展、线程化、libatomic、 libgomp、 libitm、 libquadmath、 libsanitizer、 libssp、 libvtv、 libcilkrts 和 C++ 标准库的支持。这些功能在编译交叉编译器的时候会导致编译失败，对于交叉编译 临时 libc 来说也没有必要。 –disable-multilib在 x86_64 机器上， LFS 还不支持 multilib 配置。这个选项对 x86 来说无害。 –enable-languages=c,c++这个选项确保只编译 C 和 C++ 编译器。这些是现在唯一需要的语言。 Linux-4.15.3 API Headers供系统 C 库（在 LFS 中是 Glibc）使用的应用程序编程接口（API） Glibc-2.27Glibc 软件包包括主要的 C 库。这个库提供了基本的内存分配、文件夹搜素、读写文件、字符串处理、模式匹配、算术 等等例程。 编译:12345678910../glibc-2.21/configure \ --prefix=/tools \ --host=$LFS_TGT \ --build=$(../glibc-2.21/scripts/config.guess) \ --disable-profile \ --enable-kernel=2.6.32 \ --with-headers=/tools/include \ libc_cv_forced_unwind=yes \ libc_cv_ctors_header=yes \ libc_cv_c_cleanup=yes --host=$LFS_TGT, --build=$(../glibc-2.21/scripts/config.guess)这些选项的组合效果是 Glibc 的构建系统配置它自己用 /tools 里面的交叉链接器和交叉编译器交叉编译自己。 --disable-profile编译库但不包含分析信息。如果临时工具需要分析信息则忽略此选项。 --enable-kernel=3.2这告诉 Glibc 编译能支持 Linux 3.2 以及之后的内核库。更早的内核版本不受支持。 --with-headers=/tools/include告诉 Glibc 利用刚刚安装在 tools 文件夹中的头文件编译自身，此能够根据内核的具体特性提供更好的优化。 libc_cv_forced_unwind=yes在 “Binutils-2.25 - Pass 1” 中安装的链接器是交叉编译的，在安装完 Glibc 之前不能使用。由于依赖于工作的链接器，这意味着 force-unwind 支持的配置测试会失败。将 libccvforced_unwind=yes 变量传递进去告诉configure 命令 force-unwind 支持是可用的，不需要进行测试。 libc_cv_c_cleanup=yes类似的，我们传递 libc_cv_c_cleanup=yes 到 configure 脚本跳过测试就完成了 C 清理支持的配置。 libc_cv_ctors_header=yes类似的，我们传递 libc_cv_ctors_header=yes 到 configure 脚本跳过测试就完成了 gcc 构建器支持的配置。 Libstdc++-7.3.0Libstdc++ 是标准的 C++ 库。g++ 编译器正确运行需要它。Libstdc++ 是标准的 C++ 库。g++ 编译器正确运行需要它。 编译:123456789../gcc-4.9.2/libstdc++-v3/configure \ --host=$LFS_TGT \ --prefix=/tools \ --disable-multilib \ --disable-shared \ --disable-nls \ --disable-libstdcxx-threads \ --disable-libstdcxx-pch \ --with-gxx-include-dir=/tools/$LFS_TGT/include/c++/4.9.2 –host=…指示使用我们刚才编译的交叉编译器，而不是 /usr/bin 中的。 –disable-libstdcxx-threads由于我们还没有编译 C 线程库，C++ 的也还不能编译。 –disable-libstdcxx-pch此选项防止安装预编译文件，此步骤并不需要。 –with-gxx-include-dir=/tools/$LFS_TGT/include/c++/7.3.0这是 C++ 编译器搜索标准 include 文件的位置。在一般的编译中，这个信息自动从顶层文件夹中传入 Libstdc++ configure 选项。在我们的例子中，必须明确给出这信息。 Binutils-2.30 - Pass 2编译:123456789CC=$LFS_TGT-gcc \AR=$LFS_TGT-ar \RANLIB=$LFS_TGT-ranlib \../binutils-2.25/configure \ --prefix=/tools \ --disable-nls \ --disable-werror \ --with-lib-path=/tools/lib \ --with-sysroot CC=$LFS_TGT-gcc AR=$LFS_TGT-ar RANLIB=$LFS_TGT-ranlib因为这是真正的原生编译 Binutils，设置这些变量能确保编译系统使用交叉编译器和相关的工具，而不是宿主系统中已有的。 --with-lib-path=/tools/lib这告诉配置脚本在编译 Binutils 的时候指定库搜索目录，此处将 /tools/lib 传递到链接器。这可以防止链接器搜索宿主系统的库目录。 --with-sysrootsysroot 功能使链接器可以找到包括在其命令行中的其它共享对象明确需要的共享对象。 否则的话，在某些主机上一些软件包可能会编译不成功。(….未理解) 为下一章的“再调整”阶段准备链接器：123make -C ld cleanmake -C ld LIB_PATH=/usr/lib:/libcp -v ld/ld-new /tools/bin -C ld clean告诉 make 程序移除所有 ld 子目录中编译过的文件。 -C ld LIB_PATH=/usr/lib:/lib这个选项重新编译 ld 子目录中的所有文件。在命令行中指定 Makefile 的 LIB_PATH 变量可以使我们能够重写临时工具的默认值并指向正确的最终路径。该变量的值指定链接器的默认库搜索路径。 下一章中会用到这个准备。 GCC-7.3.0 - Pass 2我们第一次编译 GCC 的时候安装了一些内部系统头文件。其中的一个 limits.h 会反过来包括对应的系统头文件 limits.h， 在我们的例子中，是 /tools/include/limits.h。但是，第一次编译 gcc 的时候 /tools/include/limits.h 并不存在，因此 GCC 安装的内部头文件只是部分的自包含文件， 并不包括系统头文件的扩展功能。这足以编译临时 libc，但是这次编译 GCC 要求完整的内部头文件。 使用和正常情况下 GCC 编译系统使用的相同的命令创建一个完整版本的内部头文件： 12cat gcc/limitx.h gcc/glimits.h gcc/limity.h &gt; \ `dirname $($LFS_TGT-gcc -print-libgcc-file-name)`/include-fixed/limits.h 准备编译 GCC:12345678910111213CC=$LFS_TGT-gcc \CXX=$LFS_TGT-g++ \AR=$LFS_TGT-ar \RANLIB=$LFS_TGT-ranlib \../gcc-4.9.2/configure \ --prefix=/tools \ --with-local-prefix=/tools \ --with-native-system-header-dir=/tools/include \ --enable-languages=c,c++ \ --disable-libstdcxx-pch \ --disable-multilib \ --disable-bootstrap \ --disable-libgomp --enable-languages=c,c++这个选项确保编译了 C 和 C++ 编译器。 --disable-libstdcxx-pch不为 libstdc++ 编译预编译的头文件(PCH)。这会花费很多时间，却对我们没有用处。 --disable-bootstrap对于原生编译的 GCC，默认是做一个“引导”构建。这不仅会编译 GCC，而且会多次编译。 它用第一次编译的程序去第二次编译自己，然后同样进行第三次。 比较第二次和第三次迭代确保它可以完美复制自身。这也意味着已经成功编译。 但是，LFS 的构建方法能够提供一个稳定的编译器，而不需要每次都重新引导。 Tcl-core-8.6.8Tcl软件包包含工具命令语言（Tool Command Language）相关程序。 此软件包和后面三个包（Expect、DejaGNU 和 Check）用来为 GCC 和 Binutils还有其他的一些软件包的测试套件提供运行支持。仅仅为了测试目的而安装 4 个软件包，看上去有点奢侈，虽然因为大部分重要的工具都能正常工作而并不需要去做测试。 尽管在本章中并没有执行测试套件（并不做要求），但是在第六章 中都要求执行这些软件包自带的测试套件。不强求为本章中所构建的临时工具运行测试套件。 Expect-5.45Expect 软件包包含一个实现用脚本和其他交互式程序进行对话的程序。 首先，强制 Expect 的 configure 配置脚本使用 /bin/stty 替代宿主机系统里可能存在的 /usr/local/bin/stty。这样可以保证我们的测试套件工具在工具链的最后一次构建能够正常。12cp -v configure&#123;,.orig&#125;sed &apos;s:/usr/local/bin:/bin:&apos; configure.orig &gt; configure 编译:123./configure --prefix=/tools \ --with-tcl=/tools/lib \ --with-tclinclude=/tools/include --with-tcl=/tools/lib这个选项可以保证 configure 配置脚本会从临时工具目录里找 Tcl 的安装位置， 而不是在宿主机系统中寻找。 --with-tclinclude=/tools/include这个选项会给 Expect 显式地指定 Tcl 内部头文件的位置。通过这个选项可以避免 configure 脚本不能自动发现 Tcl 头文件位置的情况。 DejaGNU-1.6.1./configure --prefix=/tools M4-1.4.18M4 软件包包含一个宏预处理器。./configure --prefix=/tools Ncurses-6.1123456./configure --prefix=/tools \ --with-shared \ --without-debug \ --without-ada \ --enable-widec \ --enable-overwrite --without-ada这个选项会保证 Ncurse 不会编译对宿主机系统里可能存在的 Ada 编译器的支持， 而这在我们 chroot 切换环境后就不再可用。 --enable-overwrite这个选项会告诉 Ncurses 安装它的头文件到 /tools/include 目录， 而不是 /tools/include/ncurses 目录， 保证其他软件包可以正常找到 Ncurses 的头文件。 --enable-widec这个选项会控制编译宽字符库（比如，libncursesw.so.5.9） 而不是默认的普通库（比如，libncurses.so.5.9）。 这些宽字符库在多字节和传统的 8 位环境下使用，而普通库只能用于 8 位环境。 宽字符库和普通库的源代码是兼容的，但并不是二进制兼容。 Bash-4.4.18Bash 软件包包含 Bourne-Again SHell 终端程序编译:1./configure --prefix=/tools --without-bash-malloc --without-bash-malloc这个选项会禁用 Bash 的内存分配功能（malloc）， 这个功能已知会导致段错误。而禁用这个功能后，Bash 将使用 Glibc 的 malloc 函数，这样会更稳定。 Bison-3.0.4The Bison package contains a parser generator.(解析器生成器) Bzip2-1.0.6Bzip2 软件包包含压缩和解压文件的工具。 用 bzip2 压缩文本文件比传统的 gzip 压缩比高得多。 Coreutils-8.29Coreutils 软件包包含一套用于显示和设定基本系统属性的工具。12./configure --prefix=/tools --enable-install-program=hostname--enable-install-program=hostname 这个选项会允许编译和安装 hostname 程序 – 默认是不安装的但是 Perl 测试套件需要它。 Diffutils-3.6Diffutils软件包包含用来比较文件或目录之间差异的工具。 File-5.32File 软件包包含用来判断文件类型的工具。 Findutils-4.6.0Findutils 软件包包含用来查找文件的工具。这些工具可以用来在目录树中递归查找，或者创建、维护和搜索数据库（一般会比递归查找快，但是如果不经常更新数据库的话结果不可靠）。 Gawk-4.2.0Gawk 软件包包含处理文本文件的工具。 Gettext-0.19.8.1Gettext 软件包包含了国际化和本地化的相关应用。它支持程序使用 NLS（本地语言支持）编译，允许程序用用户本地语言输出信息。12cd gettext-toolsEMACS=&quot;no&quot; ./configure --prefix=/tools --disable-shared EMACS=&quot;no&quot;这个选项会禁止配置脚本侦测安装 Emacs Lisp 文件的位置，已知在某些系统中会引起错误。 --disable-shared这次我们不需要安装任何的 Gettext 动态库，所以不需要编译。 Grep-3.1Grep 软件包包含了在文件中搜索的工具。 Gzip-1.9Gzip 软件包包含压缩和解压缩文件的工具。 Make-4.2.1Make 软件包包含了一个用来编译软件包的程序。12./configure --prefix=/tools --without-guile--without-guile 这个选项会保证 Make 不会链接宿主系统上可能存在的 Guile 库，而在下一章里通过 chroot 切换环境后就不再可用 Patch-2.7.6Patch 软件包包含一个可以通过应用“补丁”文件来修改或创建文件的程序，补丁文件通常由diff程序生成。 Perl-5.26.1Perl 软件包包含了处理实用报表提取语言（Practical Extraction and Report Language）的程序。 Sed-4.4Sed 软件包包含一个字符流编辑器。 Tar-1.30Tar 软件包包含了一个存档工具。 Texinfo-6.5Texinfo软件包包含了读写和转换info文档的工具。 Util-linux-2.31.1Util-linux 软件包包含了各种各样的小工具。12345./configure --prefix=/tools \ --without-python \ --disable-makeinstall-chown \ --without-systemdsystemunitdir \ PKG_CONFIG=&quot;&quot; --without-python这个选项会禁止使用宿主系统中可能安装了的 Python。这样可以避免构建一些不必要的捆绑应用。 --disable-makeinstall-chown这个选项会禁止在安装的时候使用 chown 命令。这对我们安装到 /tools 目录没有意义而且可以避免使用 root 用户安装。 --without-systemdsystemunitdir对于使用 systemd 的系统，这个软件包会尝试安装 systemd 特定文件到 /tools 下一个不存在的目录里。这个选项可以避免这个不必要的动作。 PKG_CONFIG=&quot;&quot;设定这个环境变量可以避免增加一些宿主机上存在却不必要的功能。请注意这里设定环境变量的方式和 LFS 其他部分放在命令前面的方式不同。在这里是为了展示一下使用 configure 脚本配置时设定环境变量的另一种方式。 Xz-5.2.3Xz 软件包包含了用于压缩和解压文件的程序。它提供了对 lzma 和更新的 xz 压缩格式的支持。使用 xz 压缩文本文件能比传统的 gzip 或 bzip2 命令有更高的压缩比。 提醒$LFS/tools 目录可以在 LFS 系统构建完成后删除，但仍然可以保留下来用于构建额外的相同版本 LFS 系统。备份 $LFS/tools 目录到底有多少好处取决于你个人如果你想保留临时工具用来构建新的 LFS 系统，现在就要备份好。本书随后第六章中的指令将对当前的工具做些调整，导致在构建新系统时会失效 到此,准备工作完成]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-07-13 lfs编译日志]]></title>
    <url>%2F2018%2F07%2F13%2F2018-07-13%2F</url>
    <content type="text"><![CDATA[lfs准备工作已经全部完成 已经把编译的工作做完了 现在准备在写md文档总结一下(时间估计两天时间) 说实话,编译的工作是真的费时间]]></content>
      <categories>
        <category>lfs编译日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux From Scrach Pass 1]]></title>
    <url>%2F2018%2F07%2F12%2Flfs1%2F</url>
    <content type="text"><![CDATA[准备准备空间大概8g 准备在u盘上构建系统我准备将u盘对半分成两个盘一个安装系统,就用ext4一个当作普通的u盘来用记得分区的时候格式化可以使用fdisk 或者 其他工具 另外: 我的arch上没有创建swap分区 挂载U盘/移动硬盘使用fdisk -l命令得到123456789Disk /dev/sdb：29 GiB，31104958464 字节，60751872 个扇区单元：扇区 / 1 * 512 = 512 字节扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节磁盘标签类型：dos磁盘标识符：0xcad4ebea设备 启动 起点 末尾 扇区 大小 Id 类型/dev/sdb4 * 63 60751871 60751809 29G c W95 FAT32 (LBA) 创建文件夹 将sdb4挂载到文件夹上 一般u盘会自动挂载,像我的系统就是挂载到run/s/xxx 下面不过官方建议还是挂载到 /mnt/lfs 下面 mount -v -t ext4 /dev/&lt;xxx&gt; $LFS # 将 /dev/ 挂载到 $LFS 如果重启设备，可能进入后发现 /mnt/lfs 目录下没有内容，这是只需要再次挂载 /dev/ 到 /mnt/lfs。 此处 用实际的设备名称代替 这里我只使用一个 / 分区 如果 LFS 使用了多个分区，(比如：一个 /，一个 /usr)，用下面的命令挂载它们： 1234mkdir -pv $LFS # 建立 / 分区的挂载点mount -v -t ext4 /dev/&lt;xxx&gt; $LFS # 将 /dev/&lt;xxx&gt; 挂载到 $LFSmkdir -v $LFS/usr # 建立 $LFS/usr 挂载点，用于挂载 /usrmount -v -t ext4 /dev/&lt;yyy&gt; $LFS/usr # 将 /dev/&lt;yyy&gt; 挂载到 $LFS/usr 附:如果你正在使用交换分区，用 swapon 命令确保它已经启用。 /sbin/swapon -v /dev/&lt;zzz&gt;用 swap 分区的名字替换。 注意:确保你的 echo $LFS 在root与用户帐号上时/mnt/lfs 你可以使用全局变量来设置 软件包和补丁mkdir -v $LFS/sources在$lfs里创建sources文件夹 来存放 下载好的软件包和补丁 设置目录的写权限和粘滞模式。“粘滞模式”意思是就算有多个用户对某个目录有写权限，仍然只有该文件的主人能删除一个粘滞目录里的文件。下面的命令可以打开写权限和粘滞模式：chmod -v a+wt $LFS/sources 下载软件包可以去中科院镜像里面去下载里面已经有了md5sums 解压放到source里面就行下载完成后使用md5sums检验====&gt;将下载的md5sums 放到sources目录下 123pushd $LFS/sourcesmd5sum -c md5sumspopd 最后的准备工作 我们还需要为构建临时系统做一些额外的准备工作。我们会在 $LFS 中新建一个文件夹用于临时工具的安装，增加一个非特权用户用于降低风险，并为该用户创建合适的构建环境。我们也会解释用于测量构建 LFS 软件包花费时间的单位，或者称为“标准编译单位（SBU）”，并给出一些关于软件包测试套件的信息 以 root 用户运行以下的命令来创建需要的文件夹：mkdir -v $LFS/tools 下一步是在宿主系统中创建 /tools 的符号链接，将其指向 LFS 分区中新建的文件夹。同样以 root 用户运行下面的命令： ln -sv $LFS/tools / 添加 LFS 用户当以 root 用户登录时，犯一个小错误可能会破坏或摧毁整个系统。因此，我们建议在本章中以非特权用户编译软件包。你可以使用你自己的用户名，但要容易的话，就建立一个干净的工作环境，创建一个名为 lfs 的新用户作为新组（名字也是 lfs ）的成员，并在安装过程中使用这个用户。以 root 用户运行以下命令来添加新用户：12groupadd lfsuseradd -s /bin/bash -g lfs -m -k /dev/null lfs 12345678910111213命令行选项的意思：-s /bin/bash把 bash 设置为 lfs 用户的默认 shell。-g lfs这个选项将用户 lfs 添加到组 lfs 中。-m为 lfs 用户创建主目录。-k /dev/null这个参数通过改变输入位置为特殊的空（null）设备，以防止可能从一个模版目录中（默认是 /etc/skel）复制文件。 lfs这是创建的组和用户的实际名称。 要以 lfs 用户身份登录（以 root 身份登录切换到 lfs 用户时不要求 lfs 用户设置了密码），需要给 lfs 用户一个密码：passwd lfs 通过更改文件夹所有者为 lfs，为用户 lfs 赋予了访问 $LFS/tools 文件夹的所有权限： chown -v lfs $LFS/tools 如果正如建议的一样创建了一个单独的工作目录，给 lfs 用户赋予这个文件夹的所有权：chown -v lfs $LFS/sources 下一步，以 lfs 用户身份登录。可以能通过一个虚拟控制台、显示控制器，或者下面的切换用户命令完成：su - lfs 设置环境123cat &gt; ~/.bash_profile &lt;&lt; &quot;EOF&quot;exec env -i HOME=$HOME TERM=$TERM PS1=&apos;\u:\w\$ &apos; /bin/bashEOF 当以 lfs 用户身份登录时，初始 shell 通常是一个可登录的 shell，它先读取宿主机的 /etc/profile文件（很可能包括一些设置和环境变量），然后是 .bash_profile 文件。.bash_profile 文件中的exec env -i.../bin/bash 命令用一个除了 HOME、TERM和 PS1 变量，完全空环境的 shell 代替运行中的 shell。这可以确保没有不必要的或者有潜在风险的环境变量从宿主机系统中泄露到构建环境。这里使用的技巧是为了有一个干净环境。 新的 shell 实例是一个非登录 shell，不会读取 /etc/profile 或者 .bash_profile文件，而是读取 .bashrc文件。 现在创建 .bashrc 文件：123456789cat &gt; ~/.bashrc &lt;&lt; &quot;EOF&quot;set +humask 022LFS=/mnt/lfsLC_ALL=POSIXLFS_TGT=$(uname -m)-lfs-linux-gnuPATH=/tools/bin:/bin:/usr/binexport LFS LC_ALL LFS_TGT PATHEOF set +h命令关闭了 bash的哈希功能。 哈希通常是一个有用的功能，bash 用一个哈希表来记录可执行文件的完整路径，以避免搜索PATH 的时间和又找到一个相同的可执行文件。然而，新工具要一安装后就使用。通过关闭哈希功能，一个程序准备运行时 shell 总是会搜索PATH变量。如此，shell 能在新编译的工具可用时马上在文件夹 $LFS/tools 中找到，而不是记录相同程序在其它地方的之前版本。 设置用户文件新建掩码（umask）为 022，确保新建的文件和目录只有它们自己的所有者可写，任何人都可读和可执行(假定open(2) 系统调用使用的默认模式是新文件使用 644模式，文件夹使用755模式)。 LFS 变量应该设置为选定的挂载点。 LC_ALL 变量控制特定程序的本地化，使得它们的消息能遵循特定国家的惯例。设置 LC_ALL 为 “POSIX” 或 “C”（两者是等价的），确保 chroot 环境中一切如期望的那样进行。 当编译我们的交叉编译器和链接器以及交叉编译我们的临时工具链时，LFS_TGT变量设置了一个非默认，但兼容的机器说明。5.2,“工具链技术说明”包含更多的信息。 把 /tools/bin 放在标准的 PATH 变量前面， 第五章中安装的软件一安装完成 shell 就可使用。这和关闭哈希功能一起，降低了在第五章环境中新程序可用时宿主机器使用旧程序的风险。 生效配置文件:source ~/.bash_profile 关于 SBUSBU 衡量方式如下。我们以第五章编译的第一个软件包 Binutils 所用时间作为一个标准编译单位（SBU），其它软件的生成时间都以其为标准进行比较。 例如，假如编译一个软件耗时 4.5 SBU，而编译安装初代 Binutils 用时 10 分钟的话，那么编译这个软件包大约耗时 45 分钟。当然啦，对于大多数人来说，编译 Binutils 可用不了 10 分钟那么久。 在一些情况下，使用多处理器同时编译可能失败，分析错误日志变得异常困难：因为不同处理器之间的执行路线是交错的。如果你在编译的时候遇到问题，那么请回过来使用单处理器编译，以正确地查看错误消息。 构建临时系统目标是生成一个临时的系统，它包含一个已知的较好工具集，该工具集可与宿主系统分离。通过使用 chroot，其余各章中的命令将被包含在此环境中，以保证目标 LFS 系统能够洁净且无故障地生成。该构建过程的设计就是为了使得新读者有最少的风险，同时还能有最好的指导价值。 最后确认一次是否正确设置了 LFS 环境变量：echo $LFS确认输出显示的是 LFS 分区挂载点的路径，在我们的例子中也就是 /mnt/lfs。 最后，必须强调两个重要的点： [重要] 重要 编译指南假定你已经正确地设置了宿主系统需求和符号链接： shell使用的是 bash。 sh 是到 bash的符号链接。 /usr/bin/awk 是到 gawk的符号链接。 /usr/bin/yacc 是到bison的符号链接或者一个执行 bison 的小脚本。 [重要] 重要 再次强调构建过程： 把所有源文件和补丁放到 chroot 环境可访问的目录，例如 /mnt/lfs/sources/。但是千万不能把源文件放在 /mnt/lfs/tools/ 中。 进入到源文件目录。 对于每个软件包: 用 tar 程序解压要编译的软件包。在第五章中，确保解压软件包时你使用的是 lfs 用户。2. 进入到解压后创建的目录中。4. 根据指南说明编译软件包。6. 回退到源文件目录。8. 除非特别说明，删除解压出来的目录和所有编译过程中生成的 &lt;package>-build 目录。 这里我在使用lfs帐号进入lfs文件夹的时候出现了权限不足,我使用了chmod加了权限 之后就是漫长的编译过程具体按照官方资料上进行编译注意在编译过程中必须清楚知道当前步骤 make 与 make install 防止错误 ====&gt;之后的构建过程 之后补充上 资料(建议配合使用)中文资料(已过期7.7,但翻译还是可以用)https://linux.cn/lfs/LFS-BOOK-7.7-systemd/index.html 英文资料(目前是8.2)http://www.linuxfromscratch.org/lfs/view/stable/index.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git443错误]]></title>
    <url>%2F2018%2F07%2F11%2Fgit443%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[在git push的时候出现OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:443 网上的解决方法都不管用 解决方法: 换网络,连上手机热点,成功 =.=]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>443错误</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git上传脚本]]></title>
    <url>%2F2018%2F07%2F11%2Fgit%E4%B8%8A%E4%BC%A0%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[由于上传命令太多了,所以写了脚本命名为comgit12#!/bin/bashgit add --all;git commit -m &quot;$1&quot;;git pull;git push; 使用方法 comgit &quot;comment&quot;]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python脚本基础]]></title>
    <url>%2F2018%2F07%2F10%2Fpython%E8%84%9A%E6%9C%AC%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[数组类型tuple字典类型dict使用*tuple解析数组使用**dict解析字典传参数的时候 单个字符,数字由*args接受,形成tuple键值对有**kwargs接受,形成dict主函数if __name__ == &#39;__main__&#39;当然也可以使用sys.argv来读取参数]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell基础]]></title>
    <url>%2F2018%2F07%2F10%2Fshell%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[shell开头12345#!/bin/bash#!/bin/sh#!/usr/bin/awk#!/usr/bin/env python#!/usr/bin/perl 这几行在第一行,不再第一行就是注释不用的话就用相关解释器带上文件名执行在写脚本的时候如果不加前面的也会交给bash解释,但是还是指定解释器比较好 注意点一定义变量名的时候等号旁边没有空格 要在登录后显示初始化内容可以将脚本文件放到 /etc/profile.d/ 的下面或者是更改/etc/motd 文件 单引号中`` 命令无效 当参数大于9个要使用${}方式 touch 主要是用来更新时间的…. 使用cat连接两个文件 ln -s 源文件 快捷方式 s表示链接 有多个命令就使用;号,若将输出全都重定向用{}扩起来,{}是父shell,()是子shell 注意点二12345678read读到换行符为止,所以就使用循环可以将文件读完,不然好像只能读一行黑洞/dev/null 了解一下-.-cp mv rm 加上-i 进行确认`command` == $(command)`read file` 是要求用户输入并且保存到file变量中...`read -p &quot;&quot; ver `就像scanf一样..(我下所的)readonly xxx=1 或者readonly xxx(已定义) 表示只读无法改变包括unset用重定向代替输入 shell中0给了stdin 1给了stdout 2给了stderr 其余数字可以来关联输出文件 例如文件中&gt;&amp;3 命令用3&gt;文件 输出到文件中 exec 将文件关联paste两个文件连接tr替换if空格[] while空格: expand unexpand 将tab 转换成空格 一般只早开头 使用-a 替换全部 unset 消除定义的变量 ${array[*]} == ${array[@]} “”${array[*]} != “${array[@]}”前面是整个数组的字符串,后面时整个数组 123456789条件语句if or elif 后面要跟上 then 如:if [ xx ]then xxxxxelif [ xxx ]then xxxxxfi 12345678[ ]是shell的内置命令 下列给出操作符:-eq equal to-ne not equal to-lt less than-gt great thange (没有-哦) great or equal如果条件判断与then写在统一行then前面加;[]两侧要有空格 注意点三 在case中 中止case语句用;;如果用;&amp; or ;;&amp;来终止还是执行下去,他中止的是子句,会使用word在去匹配pattern,直到遇到;;结束case递增可以使用let x=$x+1 当然也可以使用expr语句-.-我也是随机选择的….do 和while [ ]写在一起的时候do前面有分号while 后面可以加上命令list 使用分号;来隔开,决定是否推出循环的是最后一个命令返回值,注意是最后一个!!!其中命令都是用[]扩起来的列入[];[];[][] 是条件-n String 判断字符串长度是否非0-z Stirng 判断字符串长度为0时就断(zero)String=StringString!=String 1234567for 后面不加in就选择传入的参数同时也可以写的像c的for一样但是写成要这样for((p=1,k=321;p&lt;10;p++,k++))do xxxxxdone 1234select var in xxx xxx xxx xxxdo xxxx(可以是case或者是if条件判断)done 在select中要写退出循环的条件,不然会一直循环下去,还有就是要写其他选项的条件,不然不科学-.- option 参数的代码123456while getopts x:x:x: OPTION 这个option可以随便定义do case $OPTION in x) xxx=$OPTAGE;; 这个变量时规定的 与$REPLY一样done 写的函数可以直接调用 条件中-r 可读 -w 可写 -a and12345$&#123;xx:-dsa&#125;等:(也可以通过其他方式来实现) :-先默认后赋值,不改变 :+先赋值,不改变 :=改变 :?若是空的则输出错误退出脚本 $PAGER 似乎是系统变量 显示文档的less也可以通过其他方式来实现 echo -n 不换行输出 -e 激活转移字符 sort -u 忽略相同行 当不能使用管道时可以使用命令替代参数 数字或者字符串expression不能出现在$()中,要用()扩起来后在放到$()中间…… 循环的话要注意变量的赋值 find 命令 从给定的文件位置开始找 -iname 忽略大小写-mtime 表示修改时间 -mtime n修改时间为n天-mtime +n 大于n天-mtime -n 小于n天 如果有多个命令使用-a来连接 其中如果有()要使用\ 来转义 -o 或者 xargs 可以处理很多参数 locate 查询本地的数据库 find是直接查找文件系统 dd if=file of=file count=blocks bs=bytes 信号 通过trap xx(函数名) 数字 可以将让函数才该信号出现的时候调用该函数使用trap也可以脚本结束后调用函数,数字是0使用’’ 或者: 是屏蔽中断INT信号使用trap INT 来恢复exit xx 中xx是信号kill 进程的时候要先kill掉子进程,不然会交给init进程接管使用&amp;&amp;连接多个命令的时候是一个一个执行的,会创建子bash执行进程 command|sed &#39;command&#39;sed &#39;command&#39; file 使用;分号来连接多个command(不用另起’’) 或者使用-e(要’’分离每个命令)也可以将多个command写在一个文件中,-f指定文件!可以对取值范围取反sed stream editor 可以从管道或者文件中读取参数 sed -n &#39;1~2p&#39; 1 start 2 step sed还能加上正则/^$/使用正则还能代替行号使用&amp;表示正则表达式获取的结果 awk ‘script’ files12345678910&apos;script&apos; 中一般为 /pattern/&#123;action&#125; exprssion &#123;action&#125;其中特殊的有: value ~ /pattern/ 与正则匹配 加个!就是不匹配 可以使用()将判断分离,这样()间可以使用||或者&amp;&amp;链接分割符通过-F指定可以在awk文件中写好命令之后使用-f来指定文件使用BEGIN&#123;&#125;来执行处理数据之前执行的初始化操作使用next来表示 将两个命令的结果连接起来 #!/bin/bash/awk -f awk脚本 .awk awk中有内建变量 NR为行数 FS分割符 等等]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim基础]]></title>
    <url>%2F2018%2F07%2F10%2Fvim%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[ctrl + s 卡死后使用 ctrl + q z回车 将光标所在行移动到屏幕顶端 z. 将光标所在行移动到屏幕中间 z- 将光标所在行移动到屏幕低端]]></content>
      <categories>
        <category>vim</category>
      </categories>
      <tags>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql基本知识]]></title>
    <url>%2F2018%2F07%2F10%2Fsql%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[当我在将大小写忽略的时候之前使用大写的数据库的名字就进不去了,因为系统自动将我打的大写换成了小写….. 每一个检查点都会记录他自己的操作对象的信息状态,当共用的是同一个硬盘上的数据,硬盘上的数据是会被最近的检查点所覆盖掉的,当要回复在之前的某一个特定的检查点的时候,是先通过最近的检查点来逐级向上恢复的 当colume 与condition 条件相等时结果为result123456case colume when condition then result when condition then result when condition then resultelse resultend 当满足某一条件时，执行某一result123456case when condition then result when condition then result when condition then resultelse resultend 当满足某一条件时，执行某一result,把该结果赋值到new_column_name 字段中123456case when condition then result when condition then result when condition then resultelse resultend new_column_name case when 用在select 语句中，新的字段new_column_name可以用来排序，但是不能用在where中]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git基本操作]]></title>
    <url>%2F2018%2F07%2F10%2Fgit%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一般提交操作为:1234git add -all ==&gt; gaagit commit -m &quot;xxxx&quot; ==&gt; gcgit pull ==&gt; glgit push ==&gt; gp 自动保存密码:git config --global credential.helper store]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[css渐变动画]]></title>
    <url>%2F2018%2F07%2F10%2Fcss%E6%B8%90%E5%8F%98%E5%8A%A8%E7%94%BB%2F</url>
    <content type="text"><![CDATA[鼠标放上去渐变12345678div&#123;width:100px;transition: width 2s;-moz-transition: width 2s; /* Firefox 4 */-webkit-transition: width 2s; /* Safari 和 Chrome */-o-transition: width 2s; /* Opera */&#125; 使div居中:要设置div的宽度 再使用margin 0 auto设置]]></content>
      <categories>
        <category>css</category>
      </categories>
      <tags>
        <tag>渐变</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于pacman]]></title>
    <url>%2F2018%2F07%2F10%2Fpacman%2F</url>
    <content type="text"><![CDATA[pacman -D --asexplicite xxx 设置为单独安装的包 pacman -Qtd 出来的包可以删 但有些删除可能会有影响,大多没用 pacman -Rscn 可以删除干净包 pacman -Rdd 是在出现了依赖问题的时候用用,一般就不用它,他是强行破坏 依赖关系 有些依赖关系不一定会长久存在,所以pacman -Qtd不是删除干净包就一定没有东西的 可能会在你装包的时候出现依赖的更新 pacman不会自动帮你删除孤包 Rdd所操作的对象往往是这个包同时被其他的包依赖， 如果你正常情况下用R去卸它是会报错提示破坏依赖的。 比如一条依赖树a-b-c，即c依赖b，b又依赖a， 那么如果你用R或者Rs去卸载b就会报错提示你c的依赖将被破坏所以不能卸。 如果是用Rdd卸载b就是不管谁依赖它就是强行把b一个东西删掉， 本质上是临时破坏一下依赖。这个做法往往是出了什么问题才会用的。 卸载同时删配置文件的参数是-n，-c的意思是同时把依赖它的包也卸载掉 总结一下 对于a-b-c这样一条依赖树且a和b都是作为依赖安装的， 那么如果使用-R、-Rs去卸载b就会报错提示c的依赖被破坏并中断操作， 使用-Rdd卸载b就会强行删b，同时a和c被保留。 使用-Rc卸b就会把b和c卸载掉，用-Rsc卸b就会把abc全卸掉 ####pacman基本用法 1234567891011121314151617181920212223pacman -Sy abc #和源同步后安装名为abc的包pacman -S abc #从本地数据库中得到abc的信息，下载安装abc包pacman -Sf abc #强制安装包abcpacman -Ss abc #搜索有关abc信息的包pacman -Si abc #从数据库中搜索包abc的信息pacman -Q #列出已经安装的软件包pacman -Q abc #检查 abc 软件包是否已经安装pacman -Qi abc #列出已安装的包abc的详细信息pacman -Ql abc #列出abc软件包的所有文件pacman -Qo /path/to/abc #列出abc文件所属的软件包pacman -Syu #同步源，并更新系统pacman -Sy #仅同步源pacman -Su #更新系统pacman -R abc #删除abc包pacman -Rd abc #强制删除被依赖的包pacman -Rc abc #删除abc包和依赖abc的包pacman -Rsc abc #删除abc包和abc依赖的包pacman -Sc #清理/var/cache/pacman/pkg目录下的旧包pacman -Scc #清除所有下载的包和数据库pacman -U abc #安装下载的abs包，或新编译的abc包pacman -Sd abc #忽略依赖性问题，安装包abcpacman -Su --ignore foo #升级时不升级包foopacman -Sg abc #查询abc这个包组包含的软件包]]></content>
      <categories>
        <category>arch</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
